
------1.15.3版本
kubeadm-1.15.3-0.x86_64
kubectl-1.15.3-0.x86_64
kubelet-1.15.3-0.x86_64 依赖于socat DVD中有(可以和docker-19.03.1 版本一起使用)
kubernetes-cni-0.7.5-0.x86_64

报依赖于   conntrack  (配置 http://mirrors.huaweicloud.com/centos/7/os/x86_64/ 仓库,7是最新的7.x)
 

------CentOS安装 kubernetes v1.15.3成功
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
 
开机启动 docker
systemctl enable docker.service

sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 
sudo systemctl enable --now kubelet


CPU至少两个(两核),主机名不能有下划线,可以减号

禁用swap, 使用 swapoff -a 命令(-a,-all), 日志提示也可  --fail-swap-on=false ,即/etc/sysconfig/kubelet 的 KUBELET_EXTRA_ARGS

echo "vm.swappiness = 0">> /etc/sysctl.conf     （尽量不使用交换分区，注意不是禁用）
sysctl -p 


sudo cat /sys/class/dmi/id/product_uuid  是唯一,k8s使用这个ID在集群里标识唯一台机器 


Control-plane 节点占用端口
6443*			Kubernetes API server		
2379-2380	etcd server client API		
10250			Kubelet API	
10251			kube-scheduler	
10252			kube-controller-manager		

Worker 节点占用端口
10250				Kubelet API	 
30000-32767		NodePort Services**	 
 
确保内核netfilter被加载
lsmod | grep br_netfilter  如没有 modprobe br_netfilter 来加载

kubeadm config images pull 来验证连接 gcr.io  registries ,报连接不上 https://dl.k8s.io (被墙,可用registry.aliyucs.com/google_containers未测试)
kubeadm init   报连接不上 https://dl.k8s.io (被墙)
	--apiserver-advertise-address  IP地址被监听 ，default network interface will be used.
   --apiserver-bind-port  default 6443       
	--kubernetes-version  (default "stable-1") 配置为 v1.15.3
	--pod-network-cidr	IP地址范围为pod, CIDR（无类别域间路由，Classless Inter-Domain Routing） 设置为10.244.0.0/16 是因为flannel默认的地址 
	--service-cidr 		IP地址范围为 service VIPs. (default "10.96.0.0/12")
	还有节点网络（网段）
   --config        配置文件路径 
	--ignore-preflight-errors 可是  'IsPrivilegedUser,Swap' 也可是 all 
	 	测试 --ignore-preflight-errors=Swap 可不用在/etc/sysconfig/kubelet 中配置值 --fail-swap-on=false,但每启动还是报，所在要加

kubeadm init  --pod-network-cidr=10.244.0.0/16  --kubernetes-version=v1.15.3 

tail -f  /var/log/messages
journalctl -xeu kubelet


也可设置 HTTPS_PROXY 环境变量(docker.service)

	警告detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd"
	提示
	 Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml" 
	Using certificateDir folder "/etc/kubernetes/pki"
	
	
修改 cgroupfs 驱动到 systemd 
vi /etc/docker/daemon.json 
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
 

---vi kube-img-pull.sh
#!/bin/bash
images=(kube-apiserver:v1.15.3 kube-controller-manager:v1.15.3 kube-scheduler:v1.15.3 kube-proxy:v1.15.3 pause:3.1 etcd:3.3.10 )

for imageName in ${images[@]} ; do
   
  docker pull kontenapharos/$imageName  
  docker tag kontenapharos/$imageName k8s.gcr.io/$imageName  
# docker rmi kontenapharos/$imageName 	
done

docker pull coredns/coredns:1.3.1
docker tag  coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
#docker rmi coredns/coredns:1.3.1

docker pull quay.io/coreos/flannel:v0.11.0-amd64


docker pull siriuszg/kubernetes-dashboard-amd64:v1.10.1
docker tag  siriuszg/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
#docker rmi siriuszg/kubernetes-dashboard-amd64:v1.10.1

docker pull mirrorgooglecontainers/metrics-server-amd64:v0.3.6
docker tag 	mirrorgooglecontainers/metrics-server-amd64:v0.3.6   k8s.gcr.io/metrics-server-amd64:v0.3.6
#docker rmi mirrorgooglecontainers/metrics-server-amd64:v0.3.6

#--deprecate
docker pull heleicool/heapster-influxdb-amd64:v1.5.2
docker tag  heleicool/heapster-influxdb-amd64:v1.5.2 k8s.gcr.io/heapster-influxdb-amd64:v1.5.2
#docker rmi heleicool/heapster-influxdb-amd64:v1.5.2

docker pull netonline/heapster-amd64:v1.5.4
docker tag  netonline/heapster-amd64:v1.5.4  k8s.gcr.io/heapster-amd64:v1.5.4
#docker rmi netonline/heapster-amd64:v1.5.4

docker pull heleicool/heapster-grafana-amd64:v5.0.4 
docker tag  heleicool/heapster-grafana-amd64:v5.0.4  k8s.gcr.io/heapster-grafana-amd64:v5.0.4
#docker rmi heleicool/heapster-grafana-amd64:v5.0.4 

---
docker image save -o docker_kube-apiserver-v1.15.3.tar kontenapharos/kube-apiserver:v1.15.3
docker image save -o docker_kube-controller-manager-v1.15.3.tar kontenapharos/kube-controller-manager:v1.15.3
docker image save -o docker_kube-scheduler-v1.15.3.tar kontenapharos/kube-scheduler:v1.15.3
docker image save -o docker_kube-proxy-v1.15.3.tar kontenapharos/kube-proxy:v1.15.3
docker image save -o docker_paus-3.1.tar kontenapharos/pause:3.1
docker image save -o docker_etcd-3.3.10.tar kontenapharos/etcd:3.3.10
docker image save -o docker_coredns-1.3.1.tar coredns/coredns:1.3.1

docker image save -o docker_flannel-v0.11.0.tar  quay.io/coreos/flannel:v0.11.0-amd64

docker image save -o metrics-server-amd64-v0.3.6.tar mirrorgooglecontainers/metrics-server-amd64:v0.3.6


docker image save -o docker_kube-dashboard-v1.10.1.tar siriuszg/kubernetes-dashboard-amd64:v1.10.1
#--deprecate
docker image save -o docker_heapster-influxdb-amd64-v1.5.2.tar heleicool/heapster-influxdb-amd64:v1.5.2
docker image save -o docker_heapster-amd64-v1.5.4.tar netonline/heapster-amd64:v1.5.4
docker image save -o docker_heapster-grafana-amd64-v5.0.4.tar heleicool/heapster-grafana-amd64:v5.0.4 

--vi load-rename.sh
for f in `ls docker*` ; do docker load -i $f; done
images=(kube-apiserver:v1.15.3 kube-controller-manager:v1.15.3 kube-scheduler:v1.15.3 kube-proxy:v1.15.3 pause:3.1 etcd:3.3.10 )
for imageName in ${images[@]} ; do 
  docker tag kontenapharos/$imageName k8s.gcr.io/$imageName  
  docker rmi kontenapharos/$imageName 	
done
 
docker tag  coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
docker rmi coredns/coredns:1.3.1

docker tag  siriuszg/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
docker rmi siriuszg/kubernetes-dashboard-amd64:v1.10.1

docker tag 	mirrorgooglecontainers/metrics-server-amd64:v0.3.6   k8s.gcr.io/metrics-server-amd64:v0.3.6
docker rmi mirrorgooglecontainers/metrics-server-amd64:v0.3.6
---



-----------heapster 官方说1.11版本开始 heapster被废弃了

docker 有 stats/top 命令 
kubectl top pod 提示要有 heapster 才可用，是一个从各节点抓取信息的工具 ,dashboard的数据也来自heapster

每个节点的kubelet进程 自带一个cAdviser用来收集这个节点的全部信息（pod,container）,打开后默认监听4194端口
heapster 是一个pod,每个节点的cAdviser的数据向heapster发送数据，如heapster保存长时间的数据要用InfluxDB,使用图表工具Grafana来看 
heapster还要依赖于RBAC设置,InfluxDB应用外部分布式存储卷


看 https://github.com/kubernetes-retired/heapster/tree/master/deploy/kube-config/rbac/heapster-rbac.yaml
   可以看到 ClusterRoleBinding 绑定到 ClusterRole的一个system:heapster角色名，kubernetes系统自带，
	ServiceAccount是kube-system名称空间的heapster

安装influxdb
看 https://github.com/kubernetes-retired/heapster/blob/master/deploy/kube-config/influxdb/influxdb.yaml
	中有使用 k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 镜像
	docker pull heleicool/heapster-influxdb-amd64:v1.5.2
	docker tag  heleicool/heapster-influxdb-amd64:v1.5.2 k8s.gcr.io/heapster-influxdb-amd64:v1.5.2

	influxdb-storage 是挂的emptyDir: {}，生产环境要修改
	部署的service名字为 monitoring-influxdb (kube-system名称空间)，端口为8086，selector的值为k8s-app: influxdb 符合Deployemnt的labels值
--
	apiVersion 是老的值extensions/v1beta1，修改为现在 apps/v1,还要在顶层spec下增加selector.matchLabels 抄已有的,测试不行？？？
		selector:
		  matchLabels:
		    task: monitoring
		    k8s-app: inluxdb
---
 kubectl create -f  influxdb.yaml

kubectl get svc -n kube-system 有监听8086端口
kubectl get pod -n kube-system 
kubectl log monitoring-influxdb-xxxxxx -n kube-system

安装heapster的RBAC
kubectl create -f  heapster-rbac.yaml

安装 heapster
把文件 https://github.com/kubernetes-retired/heapster/blob/master/deploy/kube-config/influxdb/heapster.yaml 下载好
看文件 command  有连接influxdb,有Service使用80端口 ，如想在集群外仿问heapster最尾spec子级加 type:NodePort
有镜像 k8s.gcr.io/heapster-amd64:v1.5.4  
  docker pull netonline/heapster-amd64:v1.5.4
  docker tag  netonline/heapster-amd64:v1.5.4  k8s.gcr.io/heapster-amd64:v1.5.4
kubectl create -f heapster.yaml

kubectl get svc -n kube-system  看heapster随机端口，就可以直接连接所在节点IP:端口来仿问 ,报404说明已经通


安装 grafana
https://github.com/kubernetes-retired/heapster/blob/master/deploy/kube-config/influxdb/grafana.yaml
 看文件有volumns 叫ca-certificates如要https做证书，有emptyDir，生产环境要修改，有一个Service 使用端口80
 看env中INFLUXDB_HOST变量
 
 如想在集群外仿问grafana最尾spec子级加 type:NodePort
	 
 有镜像  k8s.gcr.io/heapster-grafana-amd64:v5.0.4
  docker pull heleicool/heapster-grafana-amd64:v5.0.4 
  docker tag  heleicool/heapster-grafana-amd64:v5.0.4  k8s.gcr.io/heapster-grafana-amd64:v5.0.4
kubectl create -f grafana.yaml

kubectl get svc -n kube-system  看grafana随机端口，就可以直接连接所在节点IP:端口来仿问 ,可以看到主页
提示第一步安装grafana完成 ，第二安装数据源完成，开始第三步 点 new dashboard -> Graph  (应该有默认数据，但没有，可能是deprecate的原因)
左侧设置按钮->datasource->

https://grafana.com/grafana/dashboards 上有模板下载用

kubectl top pod  这次没提示
kubectl top node 报 metrics 无效
kubectl get pod -n kube-system  
kubectl log heapster-xxxxxx -n kube-system 报错 不能从每个节点的10255端口取数据

官方说1.11版本开始 heapster被废弃了，迁移到metrics-server

删除 
kubectl delete -f  grafana.yaml
kubectl delete -f  heapster.yaml
kubectl delete -f heapster-rbac.yaml
kubectl delete -f influxdb.yaml



---------k8s-1.15.3 对应的 nginx-ingress-controller 

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml  #现在没了
会下载镜像 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.25.1 


要运行，内容是 增加一个服务ingress-nginx，Type是nodePort方式,为了可以接入集群外部
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml
 

也可用hostNetwork方式（kind:Deployment修改为kind:DaemonSet） 在中spec.template.spec中加 hostNetwork:




