金丝雀发布， 先发布一小部分，其它大部分不动，切量做测试 ，服务中断（也可以不断）， 得名由 以前旷工开矿下矿洞前，先会放一只金丝雀进去探是否有有毒气体，看金丝雀能否活下来
滚动发布，  先金丝雀，再分批多次完成发布，服务不会中断，复杂的发布工具，发布每批的数量一般是可以配置的
蓝绿发布（双服务器组），需要两倍机器资源，如果 V2 版本有问题，则对用户体验有直接影响
功能开关发布 ,需要一个配置中心 支持现代 DevOps 理念
A/B 测试,手机端的流量(高级的按地区年龄)切换到 V2 版本
影子测试 , 生产数据库复制 

------------------------------------Kubernetes  (K8s)
https://kubernetes.io/zh/

Paas

redhat 的 PaaS平台 OpenShift (收费) 基于 Kubernetes

OKD 是社区版本的 OpenShift,在OKD-4(目前beata版)版本有界面,Kiali(Istio)有提到
	(Origin Community Distribution of Kubernetes) https://github.com/openshift/okd https://www.okd.io/
	如不在云服务器上，只能Bare Metal裸机安装在FedoraCoreOS(容器系统)31版本约650M 上才行
	
mongodb 官方支持k8s (收费)
filebeat官方支持k8s

在线验证yaml的好工具
https://onlineyamltools.com/prettify-yaml

Notepad++ ->setting-> Tab Setting ->可修改tab size 默认为4，如为yaml可修改为2，复选replace by space
	打开yaml文件不会自动高亮 language->YAML,只打开yml文件会自动高亮
	但开头有空格回车换行时以tab替代
	
bluefish 开头有空格/tab 回车换行时以当前行为准，但不能高亮yaml/yml文件

intellij idea 2018.2 插件 kubernetes 
	在写键的时候有下拉提示,也可按ctrl+shift+space(Code->completion->Smart Type)来提示，会把不对键标红做提示
	没有提示的文档
	
eclipse-3.9.7 marketplace 插件 kubernetes manifest editor 0.0.2
	可以鼠标滑到有效的键上有文档提示， 做开发提示,要按快捷键才行, Edit->content assist->default (自定义的ctrl+alt+/),不会在输入时自动提示
	 
Visual Code-1.39.2 extension Kubernetes 是Microsoft发布的 说是支持Develop, deploy and debug ,支持Minikube 环境,需要kubectl 命令
	可以鼠标滑到有效的键上有文档提示 
	在写键的时候有下拉提示,左下角的manage按钮->keyboard shortcuts->搜索suggest有Trigger Suggest 默认快捷键是ctrl+space可修改
	但如果键写错不会提示



Kubernetes (开源) 基于 Docker 构建一个容器的调度服务

 
coreDNS是扩展了DNS
Dashboard 一个web界面为 k8s


pod 一组相同网络名称空间的容器，要求一个pod必须在一个节点，pod以标签来区分，k8s的最小单位,
	pod上层还有service,防止pod重建ip地址变化造成通讯问题

master(现在叫control-plane)节点的(做HA) 运行 
	apiserver(restful)
	schedular(一个pod失效选哪个节点新建pod)	
	controller-manager(对controller的调度，即一个controller失效再启新的controller,controller是用来对pod做健康检查 )
etcd 节点为control-plane服务(做HA) 存数据像zookeeper(restful)

node节点运行 kubelet , docker , kube-proxy 当节点service变化时通知 apiserver,所有其它节点的kube-proxy得到知道做更新iptables或ipvs  
CNI (container network interface)  的实现产品为 Flannel 和 Calico 的综合产品为 Canal
kubeadm 方式安装 ，每个节点要有docker 和 kublet ,control-plane节点也运行在docker上
pause  为pod有一个基础容器不用启动，用来复制网络和卷的配置

CRI（Container Runtime Interface),其中 dockershim，containerd和cri-o都是遵循CRI



https://kubernetes.io/docs/setup/release/notes/  下载二进制
2020-07-25 稳定版本为  v1.18  https://dl.k8s.io/v1.18.0/kubernetes-server-linux-amd64.tar.gz 可以下载

server 只有linux版本
client 有linux/windows版本,没有Mac版本
node   有linux/windows版本,没有Mac版本

kubernetes-client-linux-amd64.tar.gz 包中只有 kubectl 									在 kubernetes-server-linux-amd64.tar.gz也有
kubernetes-node-linux-amd64.tar.gz 包中只有	 kubeadm,kubectl,kubelet,kube-proxy 在 kubernetes-server-linux-amd64.tar.gz也有
kubernetes-server-linux-amd64.tar.gz 多的有kube-apiserver,kube-scheduler，kube-controller-manager

github上下载release分支的源码 
https://github.com/kubernetes/kubernetes/releases 
如果有docker,则make quick-release (会pull一些东西， 下不了，国内防火墙挡了)  提示 unset ${!DOCKER_*} 清除所有开头的环境变量 
如果有go ,则make ,会让系统卡死，(在gnome下运行和界面下都是)

可snap方式安装
sudo snap install kubectl --classic
kubectl version

windows 10安装docker后，界面中有一个kubernetes选项，下载k8s镜像可以用项目 
https://github.com/AliyunContainerService/k8s-for-docker-desktop 
 PowerShell 以管理员身份运行” 的 PowerShell 中执行 Set-ExecutionPolicy RemoteSigned 命令 -> .\load_images.ps1
 配置为 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.19.3
  C:\ProgramData\DockerDesktop下的service.txt 查看Docker日志;

官方也有 kubectl 命令的windows版本下载，看客户端版本 ,kubectl version --client

虚拟机要求CPU双核

------openSUSE-leap-15.2 安装 kubernetes v1.18 使用rpm和docker成功
systemctl stop 	crio.service
systemctl disable crio.service

kubeadm config images pull --cri-socket /var/run/dockershim.sock (当有 docker 和 cri-o 一起运行时)
防火墙最好关闭吧，systemctl stop/disable firewalld ， 6443 10250 

(启动docker,swapoff -a 日志提示也可  --fail-swap-on=false,即 /etc/sysconfig/kubelet 文件的 KUBELET_EXTRA_ARGS)

  
kubeadm init  --pod-network-cidr=10.244.0.0/16  --kubernetes-version=v1.18.0  --cri-socket /var/run/dockershim.sock 
--skip-phases=preflight 

rpm包安装，执行 kubeadm init  还一直卡在preflight ???，像是检查docker镜像中已经事先下好
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
运行 kubeadm config images pull  --kubernetes-version=v1.18.0 提示下载位置为 registry.opensuse.org/kubic/ 报找不到失败


kubeadm init  --pod-network-cidr=10.244.0.0/16  --kubernetes-version=v1.18.0  --cri-socket /var/run/dockershim.sock

kubeadm  reset  --cri-socket /var/run/dockershim.sock
 
journalctl  -xeu kubelet 看日志还是会下载registry.opensuse.org/kubic/下载的镜像 ,关闭防火墙
rpm 版本要打新tag 
https://registry.opensuse.org/cgi-bin/cooverview －>official按钮，点开链接选择版本，有显示docker pull 命令
--tag.sh
#!/bin/bash
images=(kube-apiserver:v1.18.0 kube-controller-manager:v1.18.0 kube-scheduler:v1.18.0 kube-proxy:v1.18.0 pause:3.2 etcd:3.4.3-0 coredns:1.6.7)
for imageName in ${images[@]} ; do
	echo "doing ${imageName} "  
  docker tag k8s.gcr.io/$imageName  registry.opensuse.org/kubic/$imageName  
done
-- 
还要加 --skip-phases=preflight ???
  
有提示(required cgroups disabled),docker是systemd, 看/var/lib/kubelet/kubeadm-flags.env 显示是systemd

--
其实还是kubelet 没有成功监听6443端口,docker的api-server没有启动???
ps -ef | grep kubelet 发现使用的是 --container-runtime-endpoint=unix:///var/run/crio/crio.sock ???
vi /var/lib/kubelet/kubeadm-flags.env(reset文件就没了),有定义KUBELET_KUBEADM_ARGS 为下面文件使用
vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf  在KUBELET_KUBECONFIG_ARGS后增加 ,在后还有是crio参数,所以还原
分析说明crio是动态放在KUBELET_EXTRA_ARGS变量中 
在 vi /etc/sysconfig/kubelet(不会被reset) 修改配置 --container-runtime=docker --container-runtime-endpoint=unix:///var/run/dockershim.sock (显示是systemd)
--修改成功

#另一节点 只需安装 kubernetes-proxy,kubernetes-kubeadm
在preflight阶段报不能是btrfs（ext4可以的）加 --skip-phases=preflight 可以成功
但也要kubelet修改为docker,要和主使用一样的cgroups

 
kubeadm join 192.168.1.102:6443 --token c5syyq.3hqpi1i4zqsotmb5 \
    --discovery-token-ca-cert-hash sha256:a05e6937f3e3e438fd7ff51ff0c2d17aad9bb9ca52f5beb904a8f487d89842ea  --skip-phases=preflight --cri-socket /var/run/dockershim.sock 
    
安装了 flannel ,kubectl get nodes 也全是NotReady 

安装 flannel (依赖 net-tools-deprecated)，flannel-k8s-yaml-0.11.0 软件包 (记得有依赖flanneld命令，应该只能是rpm)
kubectl  apply -f /usr/share/k8s-yaml/flannel/kube-flannel.yaml 会下载 registry.opensuse.org/kubic/flannel:0.11.0 镜像
docker tag  quay.io/coreos/flannel:v0.11.0-amd64  registry.opensuse.org/kubic/flannel:0.11.0
journalctl -xeu kubelet 报 
	[failed to find plugin "flannel" in path [/opt/cni/bin] failed to find plugin "portmap" in path [/opt/cni/bin

即项目 https://github.com/containernetworking/plugins , rpm -ql cni-plugins-0.8.4-lp152.1.4.x86_64  安装在/usr/lib/cni/portmap,/usr/lib/cni/flannel
两节点都做 cp /usr/lib/cni/* /opt/cni/bin (原rpm默认使用crio) ，终于主节点Ready了

排错方法
kubectl get pod -n kube-system
kubectl describe pod kube-flannel-ds-amd64-xxxx -n kube-system
kubectl describe pod  coredns-xxx  -n kube-system 



-----用linux二进制包,kubelet 放PATH环境变量中,未成功??? 
su - root
二进制包报 检查 http://localhost:10248/healthz 失败,即内部启动kubelet没有成功 ???  日志提示systemctl status kubelet ,journalctl -xeu kubelet，可能用二进包不行的
---/usr/lib/systemd/system/kubelet.service (/etc/systemd/system/multi-user.target.wants/kubelet.service → )
[Unit]
Description=kubelet: The Kubernetes Node Agent
Documentation=https://kubernetes.io/docs/
After=network.target network-online.target
Wants=docker.service crio.service

[Service]
ExecStartPre=/bin/bash -c "findmnt -t bpf --mountpoint /sys/fs/bpf || mount bpffs /sys/fs/bpf -t bpf"
ExecStart=/usr/bin/kubelet
Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target

---

/etc/containers/registries.conf 默认是从docker.io
#kubelet  # --cgroup-driver=systemd  (default "cgroupfs" 要和docker 一样才行)

kubectl version --client
kubectl cluster-info dump 连接到 Kubernetes cluster 

docker container  ls 
docker container logs  CONTAINERID

------------------CentOS
centos 8 自带的podman-docker虽然也有docker命令，但k8s不认的 ，要删除runc ,yum remove runc ,删所有依赖
centos 8 安装k8s可用7的仓库 
 
安装后第一个命令kubectl get nodekub 报 try setting KUBERNETES_MASTER environment variable (可能是没有运行kubeadm init 成功后的命令)
	export KUBECONFIG=/etc/kubernetes/admin.conf 即可，可写入/etc/profile下


国内kubernetes镜像
---vi /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64  
#原始官方地址为 https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
---
setenforce 0
yum install  kubelet kubeadm kubectl

如rpm -ivh *kube*.rpm 报 not an rpm package 用yum install *.rpm 就可以


---安装前检查 
虚拟机要双CPU 
CentOS 系统  setenforce 0 
关防火墙（上面要开好多端口）

vi /etc/sysctl.d/k8s.conf 增加（为防止报错才加）
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1  #这个错误会提示
net.ipv4.ip_forward=1			#这个错误会提示

执行 sysctl --system
cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 为1
cat /proc/sys/net/bridge/bridge-nf-call-iptables 为1  
cat /proc/sys/net/ipv4/ip_forward 要为1
 

----1.18.0 版本  
yum install  kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 安装指定版本
	如docker版本过高,kubeadm以warning会提示 k8s-1.18.0最高支持 docker-ce-19.03

报依赖于  
conntrack-tools
#conntrack  (配置 http://mirrors.huaweicloud.com/centos/7/os/x86_64/ 仓库,7是最新的7.x)

cri-tools
kubernetes-cni
libnetfilter_cthelper
libnetfilter_cttimeout
libnetfilter_queue
socat （DVD中有的）

kubeadm config images pull 测试容器下载（kubeadm init 会提示）

提示从https://k8s.gcr.io/v2/下载docker镜像失败 
k8s.gcr.io/kube-apiserver:v1.18.0
k8s.gcr.io/kube-controller-manager:v1.18.0
k8s.gcr.io/kube-scheduler:v1.18.0
k8s.gcr.io/kube-proxy:v1.18.0
k8s.gcr.io/pause:3.2
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.7


docker pull codedingan/kube-apiserver:v1.18.0
docker pull codedingan/kube-controller-manager:v1.18.0
docker pull codedingan/kube-scheduler:v1.18.0
docker pull codedingan/kube-proxy:v1.18.0
docker pull codedingan/pause:3.2
docker pull codedingan/coredns:1.6.7
docker pull codedingan/etcd:3.4.3-0


---vi kube-img-bak_restore.sh
#!/bin/bash
images=(kube-apiserver:v1.18.0 kube-controller-manager:v1.18.0 kube-scheduler:v1.18.0 kube-proxy:v1.18.0 pause:3.2 etcd:3.4.3-0 coredns:1.6.7)
for imageName in ${images[@]} ; do
	echo "doing ${imageName} " 
  #docker pull codedingan/$imageName  
  docker tag codedingan/$imageName k8s.gcr.io/$imageName  
  #docker rmi codedingan/$imageName
  docker image save -o docker_kube-1.18.0_${imageName}.tar  k8s.gcr.io/$imageName
done
#for f in `ls docker*` ; do docker load -i $f; done
----

第一次启动kubelet 报没有 /var/lib/kubelet/config.yaml 忽略（没有kubeadm init/join）
kubeadm init  --pod-network-cidr=10.244.0.0/16  --kubernetes-version=v1.18.0 (测试成功)日志会自动启动kubelet,执行成功后 systemctl stop/start kubelet 就可成功,提示systemctl enable kubelet.service

但另一台机器 执行 kubeadm join  双方都关闭防火墙和selinux(setenforce 0 ) 成功

kubectl get nodes  状态是NotReady要安装网络

-- 新的flannel 为 k8s-v1.7+ 
https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml
kubectl apply -f  kube-flannel.yml

docker image save -o kube_flannel-0.12.tar  quay.io/coreos/flannel:v0.12.0-amd64 
docker load -i kube_flannel-0.12.tar

-- 新的 metrics-server-0.3.x  支持k8s-1.8+
https://github.com/kubernetes-sigs/metrics-server

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml

docker pull bitnami/metrics-server:0.3.7
docker tag  bitnami/metrics-server:0.3.7  k8s.gcr.io/metrics-server/metrics-server:v0.3.7

docker image save -o kube_metrics-server-0.3.7.tar  k8s.gcr.io/metrics-server/metrics-server:v0.3.7
docker load -i  kube_metrics-server-0.3.7.tar 


kubectl get svc -n kube-system
kubectl get pod -n kube-system

--- 新的 ingress-nginx 为 k8s-v1.18.0 
docker image save -o kube-webhook-certgen-v1.5.0.tar  jettech/kube-webhook-certgen:v1.5.0
docker load -i  kube-webhook-certgen-v1.5.0.tar

docker pull pollyduan/ingress-nginx-controller:v0.41.2
docker tag pollyduan/ingress-nginx-controller:v0.41.2  		k8s.gcr.io/ingress-nginx/controller:v0.41.2
docker image save -o ingress-nginx-controller-v0.41.2.tar   k8s.gcr.io/ingress-nginx/controller:v0.41.2
docker load -i 	ingress-nginx-controller-v0.41.2.tar
----

kubeadm init 成功提示
 
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
	
	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" 

	Then you can join any number of worker nodes by running the following on each as root:
	kubeadm join <本机外网IP>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<long-hash> 
------------

如 kubeadm init 或 kubeadm join 失败，一些文件已经再次就会提示文件存在的错误
使用 kubeadm reset 还要 rm -rf ~/.kube/config
 kubeadm reset 提示清iptables 使用 iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X)

sudo kubeadm token list  
会显示init时的 --token 的值，但没有 --discovery-token-ca-cert-hash的值


cat  /var/lib/kubelet/kubeadm-flags.env
	KUBELET_KUBEADM_ARGS="--cgroup-driver=cgroupfs"
	KUBELET_KUBEADM_ARGS="--cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1"

cat /etc/sysconfig/kubelet 
	KUBELET_EXTRA_ARGS=
	可配置值 --fail-swap-on=false

kubeadm join 成功后提示 control-plane 节点运行 kubectl get nodes 有列ROLES显示master
 
显示各组件运行状态 
kubectl get cs (cs=componentstatus) 
kubectl get nodes 节点信息
发现control-plane(现在是master) 状态是NotReady ，因少flannel这种网络组件

 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
 要等 
 
docker image ls    有 quay.io/coreos/flannel:v0.11.0-amd64 镜像

#docker image save -o docker_flannel-v0.11.0.tar  quay.io/coreos/flannel:v0.11.0-amd64


kubectl get pods -n kube-system 有 kube-flannel-ds-amd64- 结果 -n名称空间
kubectl get nodes 发现control-plane(现在是master) 状态是Ready

kubectl get ns 查所有名称空间


在从节点 kubeadm join 后要等一会，下载flannel，kube-proxy镜像，启动容器，
在主节点查kubectl  get pods  -n kube-system -o wide 可以看到两个proxy两个flannel，各在哪个节点上

kubectl describe node  node0  有Label信息， 资源分配情况
kubectl cluster-info


kubectl run   --help有很多示例命令 
	--dry-run 显示将发送，但不发送
	--restart=Never 如不加，当pod丢失，会自动补一个
	--command -- <cmd> <arg1> ... <argN> 覆盖默认启动命令 
	-i -t 和docker功能相同，交互式
	
kubectl run  nginx-deploy --image=nginx:latest  --port=80  
(kubectl get pods 可查)
#Flag --replicas has been deprecated, has no effect and will be removed in the future.
＃Flag --generator has been deprecated, has no effect and will be removed in the future.
kubectl delete pods nginx-deploy

kubectl create deployment nginx-deploy --image=nginx:latest 

	
kubectl get deployment  (可简写为deploy)就可以看到建的 要等一会AVAILABLE状态才为1
	-w 表示watch 相当于linux tail
kubectl get pods -o wide 就有了 显示所在IP,主机名，进入主机查网卡用的是cni0 的桥（网卡） ip为 10.244.2.1，netmask是24位，此子网只为这个从节点上的pod使用
curl  <IP> 可仿问
本机有cni0网卡

kubectl delete pods  <POD_NAME>，因get deploy显示的Desired(Ready列的/后的)为1,会自动重新 后再查，发现Name变了，Status为ContainerCreating，IP地址变了，所在节点也可能有变化

使用固定不变的service来仿问pod

 给名为nginx-deploy的deployment创建 service名为nginx
kubectl expose deployment nginx-deploy --port=80 --target-port=80 --name=nginx --protocol=TCP
	--port=<servie端口> 
	--target-port=<容器端口> 
	--name=<service名>  
	--type=ClusterIP 默认的只服务于集群内 ,NodePort,LoadBanacer

kubectl get svc  
kubectl get services  显示的IP地址，10.103.x.x是10.96.0.0/12的子网地址，只可集群内可仿问到
 service用名仿问代替IP ，要用coreDNS
kubectl  get pods  -n kube-system -o wide  查coreDNS 的IP(pod层），CoreDNS也有服务名
kubectl  get svc   -n kube-system    显示名为kub-dns的IP 10.96.0.10(serivce层的),容器中/etc/resolve默认配置成这个IP
yum install bind-utils (为dig)
dig -t A  nginx @10.96.0.10 #centos不行,openSUSE下可以的

kubectl run  busybox-client --image=busybox:latest  -it --restart=Never
>cat /etc/resolv.conf 显示指向了coreDNS
   nameserver 10.96.0.10
   search   default.svc.cluster.local    svc.cluster.local    cluster.local
   #default表pod所属名称空间     svc.cluster.local表k8s本地pod资源
	里面 ping nginx / wget nginx 是可用的 (curl 不行，因search会自动加default.svc.cluster.local) 是可解析service名为nginx的

dig -t A  nginx.default.svc.cluster.local @10.96.0.10  可以成功解析

看这个serivce/deployment关联哪些pods,标签选择器
kubectl  describe  svc  nginx
	Endpoint: 的值是连接到pod所在节点
	Selector: app=nginx-deploy 标签选择器
	IP : 这个servie的IP

kubectl  get pods  --show-labels   显示标签 和上面的service中的一样的

kubectl edit svc nginx 打开vi 编辑yaml文件，可修改clusterIP:的值， 不行 在注释区提示spec.clusterIP field is immutable 
删了重建
kubectl delete svc nginx  
kubectl expose deployment nginx-deploy --port=80 --target-port=80 --name=nginx --protocol=TCP 重建后service地址变了 
	pod中还是可以使用service名为nginx来仿问的

kubectl  describe  deployment nginx-deploy  有 Selector: run=nginx-deploy同，中定义Replicas:  1 desired

kubectl scale --replicas=3 deployment nginx-deploy 动态修改pod数量 （修改deployment）
再请求时，多个pod来回切换处理请求 while true; do wget -O - -q myapp/hostname.html; sleep 1 ; done

kubectl  describe  pods <pod_name>  有Containers: 组下的 <容器名> 为nginx，每个容器有镜像文件

动态修改镜像版本（升级版本）
kubectl set image deployment nginx-deploy  nginx=nginx:1.17.5-alpine   #<容器名>=镜像

kubectl rollout status deployment nginx-deploy 显示更新版本过程

回滚上一个版本 
kubectl rollout undo deployment nginx-deploy 


iptables -vnL  有很多规则 有IPVS规则 -v verbose
iptables -vnL -t nat

如想集群外部仿问，只要修改service的Type为NodePort
kubectl edit svc nginx 打开vi 编辑yaml文件， 修改clusterIP的值不行，修改type 的值为NodePort为外网仿问(原值为ClusterIP)
再用 kubectl get svc 看服务已经有Port列 有80: 后面有端口了，表示每个节点的这个端口可以用外网来仿问了

对外仿问 如service挂了（再重启端口可能会变）只能外加一层proxy 做HA 了

#kubectl delete deploy nginx-deploy
#kubectl delete svc nginx  
#kubectl delete pod busybox-client


-------
资源对象 
工作负载类：Pod,ReplicaSet,StatefulSet,DemonSet,Job,CronJob
服务及发现:Service,Ingress
存储： Volume,ConfigMap,Secre
集群级：NameSpace,Node,Role,ClusterRole,RoleBinding,ClusterRoleBinding
#元数据：PodTemplate,LimitRange
-------

kubectl  get pods 的name列值 xxx
kubectl  get pod xxx -o yaml  以yaml显示，观察格式
					-o name 只输出名字（资源类型/资源名）
kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image

api-server只接收json,会把yaml转换为json,是restful api

kubectl api-versions  显示所有yaml用的apiVersion版本

.yaml格式文件  (manifest) 一级字段
 	apiVersion 
	kind 类型如pod
	metadata 子级的name，同一类别中要唯一，namespace，labels,annotations(不能用于挑选资源对象，可以被程序使用的元数据)
	spec 用户期望的，户定义的目标状态，针对kind不同spec就不同
	status 当前的状态(只读的)，向spec靠近 ,kubernetes来修改
	
kubectl explain pods  显示定义pod要有什么  
kubectl explain pods.metadata 
kubectl explain pods.spec  
如帮助有-required-是必填的 

kubectl explain pods.spec.containers
	imagePullPolicy <> 可选值有Always ,IfNotPresent，Never 
		默认值如tag为:latest则为Always,否则为IfNotPresent ，创建后不能修改
	ports  只是一个信息知道会有哪些端口可用,如不指定值也不能阻止已经监听端口暴露
	command 给的命令是不会行的在shell中的，如果没指使用Docker镜像中的entrypoint
	args 如果没指使用Docker镜像中的cmd,变量写法$(X),是不同于shell${X}

	如果只提供command,没有args ,会忽略Docker容器中的entrypoint和cmd   ( Docker容器中 的cmd做为参数转给 Docker容器中entrypoint)
	如果只提供args,没有command ,会把args给Docker容器中的entrypoint(忽略Docker容器中的cmd)
	
---vi 1-pod-demo.yamlapiVersion: v1
kind: Pod
metadata:
  name: pod-demo
  namespace: default
  labels:
    app: myapp
    tier: frontend
  #annotations:
  #  created-by: cluster admin
spec:
  containers:
  - name: myapp
    image: nginx:latest
  - name: busybox
    image: busybox:latest
    #imagePullPolicy: IfNotPresent
    command:
    -  "/bin/sh"
    -  "-c"
    -  "sleep 3600"
  #restartPolicy: #可取值为Always,OnFailure,Never
  #nodeSelector:
  # diskType: ssd
  #vscode提示没有resource limits 
    
-----
			
		
---
kubectl create -f pod-demo.yaml #里有myapp容器
kubectl  get pods 显示创建的 pod-demo ,READY列显示共有几个容器，几个是好的
	-w (watch)
kubectl describe pods pod-demo
kubectl logs  pod-demo myapp  相当于看窗口的控制台

kubectl exec -it pod-demo -c myapp -- /bin/sh  (-c container)


kubectl delete -f 1-pod-demo.yaml #按配置文件中资源做删除，不会被controller管理，不会自动创建


kubectl get pods --show-labels  显示所有标签


label长度有限制，命名也有限制

kubectl get pods -L app    	增加一标签列为app，显示每个pod的对应的值
kubectl get pods -L app,release	可以加多列用,分隔

kubectl get pods -l app   --show-labels 	显示有app标签的，（不管值)

kubectl get pods -l release=stable   --show-labels 
kubectl get pods -l release=stable,app=myapp  是并且的关系
kubectl get pods -l release!=stable  (没有那个标签 也是!=)

kubectl get pods -l "release in (canary,beta)"
kubectl get pods -l "release not in (canary,beta)"
 
  !KEY 不存在此标签

	{key:"KEY","operator":"OPERATOR","values":[VAL1,VAL2]}
	OPERATOR 可取值为In,NotIn 要求valuels必须有值
					Exists,NotExists 要求vlues必须为空
	
	
pods 打标签
kubectl label pods pod-demo release=canary   (canary 金丝雀)
kubectl label pods pod-demo release=stable --overwrite

给node 打标签
kubectl label node node10 disktype=ssd



kubectl explain pods.spec   
	有  nodeSelector,nodeName(pod只能运行在这个节点上)

资源注解annotation和label类似，只是不能被选中，只是元数据，长度没有限制，可动态编辑

kubectl describe pods pod-demo
有Annotation字段



kubectl explain pods.spec.containers.livenessProbe     容器是否健康
	exec(有command),httpGet(port可以使用名字),tcpSocket 三种检查方式
	failureThreshold 默认3次，只3 连续3次失败算失败
	periodSeconds 每次检查的间隔，默认10s
	timeoutSeconds 每次多长时间超时, 默认1s		
	initialDelaySeconds 容器启动多长时间开始检查 

kubectl explain pods.spec.containers.lifecycle   	容器刚开始启动时可以加入自定义，启动前入自定义（可串行启动多个其它容器），结束前加入自定义
kubectl explain pods.spec.containers.readinessProbe     容器是否准备好 

---vi 2-liveness-exec.yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-exec
  namespace: default
  labels:
    app: myapp
    tier: frontend
  #annotations:
  #  created-by: cluster admin
spec:
  containers:
  - name: busybox
    image: busybox:latest
    imagePullPolicy: IfNotPresent 
    command: ["/bin/sh","-c","touch /tmp/healthy;sleep 30 ; rm -f /tmp/healthy;sleep 360"]
    livenessProbe:
      exec:
        command: ["test","-e","/tmp/healthy"]
      initialDelaySeconds: 1
      periodSeconds: 3
  #restartPoloicy: #可取值为Always,OnFailure,Never
  #nodeSelector:
  #  diskType: ssd
  
---

kubectl describe pod liveness-exec 看Liveness
kubectl get  pod  有RESTARTS 列 重启次数


---vi 3-liveness-httpget.yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-httpget
  namespace: default
  labels:
    app: myapp
    tier: frontend
  #annotations:
  #  created-by: cluster admin
spec:
  containers:
  - name: myapp
    image: nginx:latest  
    ports:
    - name: http
      containerPort: 80
    - name: https
      containerPort: 443
    livenessProbe:
    #readinessProbe:
      httpGet:
        port: http #是前面定义的端口名
        path: /index.html
      initialDelaySeconds: 1
      periodSeconds: 3
-----
kubctl get pods 显示有 liveness-httpget-pod
kubctl exec -it liveness-httpget-pod -- /bin/sh

kubectl explain pods.spec.containers.lifecycle  
	里有preStop,postStart    在pod启动后，终止前执行的操作
kubectl explain pods.spec.containers.lifecycle.postStart

#kubectl delete pod  liveness-httpget

---vi 4-poststart-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: poststart-pod
  namespace: default
  labels:
    app: myapp
    tier: frontend
  #annotations:
  #  created-by: cluster admin
spec:
  containers:
  - name: busybox
    image: busybox:latest
    imagePullPolicy: IfNotPresent 
    command: ["/bin/httpd"]
    args: ["-f"]
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh","-c","mkdir -p /data/web/html"]
    #restartPoloicy: #可取值为Always,OnFailure,Never

-----
kubectl delete -f  4-poststart-pod.yaml 删除pod后就不会自创建（自主式），不是被controller管理的，如前面的deployement就是自动创建


Deployment (管理无状态应用)在 ReplicaSet （对Pod数量做动态加或减）之上，一个Deployment可管理多个ReplicaSet
DaemonSet 集群中每个节点(也可是选择器选中的节点) （和某类pod没关，一个节点不支持运行两个）只运行一个(如日志收集）
Job 如任务正常完成，就会不用再次启动，
CronJob 周期性任务
StatefulSet 有状态应用 ,如redis,mysql（要为每个应用redis/mysql要单独准备脚本，可以使用helm，或者不放在k8s上）

建议不直接使用ReplicaSet，而是使用Deployment （支持滚动更新，加回滚）

kubectl explain rs (replica set)
kubectl explain rs.spec
	有核心的3个属性 replicas ,selector,template 
kubectl explain rs.spec.template 里面就是前面pod的的内容

kubectl get deploy
kubectl delete deploy xx


模板中创建的标签要符合标签选择器的条件
--vi 5-rs-demo.yamlapiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp  #删以前的myapp
  namespace: default
spec:
  replicas: 2 
  selector:
    matchLabels:
      app: myapp
      release: canary
  template: #的内容就是以前的pod的内容
    metadata:
      name: myapp-pod
      labels:
        app: myapp #要符合上面的 selector
        release: canary
        enviroment: qa
    spec:
      containers:
      - name: myapp-container
        #可以做存活检查
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
				    
					
--
kubectl create -f rs-demo.yaml
kubectl get rs  (rs=ReplicaSet)
kubectl get pod -o wide 显示 myapp-xxx 名共2个 可以在外部仿问显示的IP
kubectl delete pod myapp-xxx 后会自动重新启一个
kubectl get pod --show-labels  其它pod加标签，多的pod会动删，应在上一层加service

也可支持升级版本，扩/缩容
kubectl edit rs myapp 会打开vi编辑yaml文件，可以修改 replicas 数量做扩/缩容，立即生效
也可更新升级，修改容器镜像(nginx:1.17.5-alpine)
kubectl get rs  -o wide 显示修改后的镜像版本，但测试 kubectl describe pod 还是原来的版本，只有pod重建(基于修改的文件模板)才是新版本

deployment的回滚基于replicaSet实现的

一个 deployment 简写为deploy 基于多个replicaSet ，可充许最多几个pod和最少几个pod (防止删一个pod来更新导致少一个pod不能承受并发数)，如replicas为5最多可多5个就能实现蓝绿部署，
kubectl explain deploy
VERSION用 apps/v1
kubectl explain deploy.spec
	有 minReadySeconds 同前面replicaSet(为应用启动时间做准备)，有 strategy，revisionHistoryLimit 最多保存多少个历史版本，一般2个足以，pause暂停
kubectl explain deploy.spec.strategy
			type值可为 Recreate(重建更新) , RollingUpdate(默认的)
kubectl explain deploy.spec.strategy.rollingUpdate (只对type值为RollingUpdate时有效) 有 
		maxSurge (surge汹涌) 最多允许超出多少个 可是一个数，也可以是%，默认25%
		maxUnavailable  默认25% 最多有几个不可用
		上两个配置不能同时为0
		
--vi 6-deploy-demo.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 2
  selector: 
    matchLabels:
      app: myapp #删除kubectl delete rs myapp
      realease: canary
  strategy: #默认配置
    type: RollingUpdate
    rollingUpdate: 
      maxSurge: 25%
      maxUnavailable: 25% 
  template:
    metadata: 
      labels:
        #要匹配上面的标签选择器
        app: myapp  
        realease: canary 
    spec:
      containers:
      - name: myapp
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
--
kubectl apply -f deploy-demo.yaml  (apply表示即可创建也可更新)
kubectl get deploy
kubectl get rs   
	显示名字格式 myapp-deploy-xxx 即deploy名-模板hash值
	
kubectl get pod 是基于rs的名字加随机  
	可修改deploy-demo.yaml 的replicas 再次执行 kubectl apply  -f deploy-demo.yaml 查pod数变了
	
kubectl describe  deploy myapp-deploy	自动维护Annotaions 
	StrategyType: RollingUpdate
	RollingUpdateStrategy:  25% max unavailable, 25% max surge

kubectl get pods -l app=myapp -w 
可修改deploy-demo.yaml image的版本(nginx:1.17.5-alpine)  再次执行 kubectl apply  -f deploy-demo.yaml 查 镜像版本

可以用kubectl set image  来修改镜像，也可用kubectl patch 传json来修改
kubectl get rs   -o wide 有历史版本,CURRENT，DESIRED都是0,不为0的是当前版本
kubectl rollout history deployment myapp-deploy 看滚动历史记录 
#kubectl rollout --help 有子命令undo,pause

patch方式修改.yaml中的内容
kubectl patch --help 有示例
kubectl patch deployment myapp-deploy  -p '{"spec":{"replicas":5}}'

kubectl patch deployment myapp-deploy  -p '{"spec":{"strategy":{"rollingUpate":{"maxSurge":1,"maxUnavailable":0}}}}'  ##提示(no change)????
kubectl describe deployment myapp-deploy 查看确实没有必变

修改镜像,加新一个后立即暂停 就是 金丝雀发布
kubectl set image deployment myapp-deploy myapp=nginx:1.17.5-alpine  && kubectl rollout pause deployment myapp-deploy  
kubectl rollout status deployment myapp-deploy 可监视,日志显示部分更新了
如发现金丝雀没有问题继续更新
kubectl rollout resume deployment myapp-deploy


回滚上一个版本 
kubectl rollout undo deployment myapp-deploy  --to-revision=1 #1是 kubectl rollout history deployment myapp-deploy 中的revision值
1-2-3 如从3回滚到1（后变为4）,如1（变为4）再回滚就是3


--------DaemonSet
一个节点只运行一个pod

kubectl explain ds.spec

-----vi 7-ds-demo.yaml 
#这个redis用于接收filebeat的数据,教程也未成功写入数据
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: default
spec: 
  replicas: 1
  selector: 
    matchLabels:
      app: redis  
      role: logstore 
  template:
    metadata: 
      labels:
        #要匹配上面的标签选择器
        app: redis  
        role: logstore 
    spec:
      containers:
      - name: redis
        image: redis:latest
        ports:
        - name: redis
          containerPort: 6379
--- 
#一个配置文件中定义多个资源使用3个-分隔
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: filebeat
      realease: stable
  template:
    metadata:
      labels:
        #要匹配上面的标签选择器
        app: filebeat
        realease: stable
    spec:
      containers:
      - name: filebeat
        image: ikubernetes/filebeat:5.6.5-alpine
        #image: docker.elastic.co/beats/filebeat:7.10.0
        env:
        - name: REDIS_HOST
          value: redis.default.svc.cluster.local
          #是pod的服务名，redis是服务名，default 是名称空间，svc.cluster.local内建的域名后缀
        - name: REDIS_LOG_LEVEL
          value: info
      #hostNetwork:
------

filebeat（可读本地日志文件） 在logstash 官方教程有使用

#pods和podf都可以

kubectl explain pods.spec.containers.env

kubectl logs <pod_name>


kubectl apply -f ds-demo.yaml
kubectl expose deployment redis  --port 6379

kubectl get pods 查redis的Name ,redis-xxx--yyy
kubectl exec -it redis-xxx--yyy -- /bin/sh
>nslookup redis.default.svc.cluster.local  是可以解析的

master也支持运行daemonSet但因为有污点
DaemonSet也支持滚动更新
kubectl explain ds.spec 有updateStrategy 的type 有  RollingUpdate 或 OnDelete 
kubectl explain ds.spec.updateStrategy.rollingUpdate 有 maxUnavailable

kubectl describe ds filebeat
也支持动态更新版本
kubectl set image daemonsets filebeat-ds filebeat=ikubernetes/filebeat:5.6.6-alphine  #daemonsets缩写为 ds 容器=镜像 


kubectl explain pods.spec 有 hostNetwork


----------Service
三大核心资源Pod,Service,Controller(Ingress)

依赖于coreDNS

kubectl get svc 有一个kubernetes 不能删
kubectl explain svc.spec
	port 用哪个端口与容器端口关联,
	selector 关联哪些pod
	clusterIP 以10.96开头，集群内部使用
	type 默认 ClusterIP，NodePort 集群外用，为了多节点处理，可在外层加负载均衡
		LoadBalancer 要求运行在云环境中，k8s调用底层的iaas来创建出一个lbaas (openStack支持,有点大，路径有点长)负载均衡器来分发到每个节点的nodePort
		ExternalName（为pod可以和集群外部做通讯，通过service ）spec中有一个externalName，是一个别名CNAME，要能被coreDNS解析为公网域名，再到公共上解析
	
kubectl explain svc.spec.ports
	nodePort:	当type 为  NodePort时用，主机上端口，外部可以仿问
	port:  是service的端口
	targetPort： pod上的端口
	
--vi 8-redis-svc.yaml   (ClusterIP) 标签选择pod
#依赖于 7-ds-demo.yaml 的redis
apiVersion: v1
kind: Service
metadata:
	name: redis
	namespace: default
spec: 
   selector:  
		app: redis  
		role: logstore 
   clusterIP: 10.97.97.97 #可不指定动态分配，指定容易冲突
   type: ClusterIP
   ports: 
   - port: 6379 
     targetPort: 6379  
	 
--

kubectl  describe  svc  redis
显示Endpoints是pod的地址(如显示为<none>的原因没有选择中pod)，Service 和Pod有一个中间层Endpoint
每个服务建立后会在coreDNS中增加资源记录
格式 SERVICE_NAME.NAMESPACE.DOAMIN_SUFFIX  
DOAMIN_SUFFIX 默认为 svc.cluster.local

	
--vi 9-myapp-svc.yaml  (NodePort)
#依赖于 6-deploy-demo.yaml

apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec: 
  selector:  
    app: myapp  
    release: canary 
  clusterIP: 10.99.99.99  
  type: NodePort
  ports: 
  - port: 80 
    targetPort: 80  
    nodePort: 30080 #可以不定指，会动态分配
  #sessionAffinity: ClientIp 
  #默认值None,(Affinit密切关系，姻亲关系),同一客户端请求后端使用同一个pod处理
		
--
kubectl get svc 看到端口对应关系,显示为 80:30080/TCP  ,service端口：node端口

外部机器  curl  http://172.20.0.66:30080/hostname.html ，172.20.0.66是node1节点  ,节点30080端口有监听
#测试nginx主页不能仿问？？？防火墙没开？？，标签对的，pod可仿问,service转发有问题
	看service为pod做负载均衡,pod可能和servcie不在同一个节点
kubectl patch svc myapp  -p '{"sepc":{"sessionAffinity":"ClientIP"}}' ##提示 (no change) ？？？
kubectl edit svc myapp 打开vi来修改后，再查kubectl describe svc myapp就生效了,但连接节点的30080还是不行？？？


无头的Service(即clusterIP没有值,type:ClusterIP)表示请求不经Service直接到pod

--vi 10-myapp-svc-none.yaml 标签依赖于 deploy-demo.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc-none
  namespace: default
spec: 
  selector:  
    app: myapp  
    release: canary 
  clusterIP: None
  type: ClusterIP
  ports: 
  - port: 80 
    targetPort: 80   		
--
建立后 查kubectl get svc 要没有clusterIP才是对的

master节点 dig -t A myapp-svc-none.default.svc.cluster.local. @10.96.0.10   

10.96.0.10 是 kubectl get svc  -n kube-system 显示的kube-dns的IP

dig 出来的多个IP 可以和  kubectl get pods  -o wide -l app=myapp  做比较


--启用ipvs 未试 
vi /etc/sysconfig/kubelet 
	KUBELET_EXTRA_ARGS="" 
	KUBE_PROXY_MODE=ipvs  也可是EXTRA_ARGS中的选项
	启动kubelet服务时加入模块 ip_vs,ip_vs_rr,ip_vs_wrr,ip_vs_sh,nf_conntrack_ipv4
	
--------Ingress Controller
flannel.1的网卡IP就是pod的网段地址，每个节点可以和pod通讯
Service根据标签选择器查找pod，是一直watch的api-server的(etcd)数据变化

为外部使用https，集群内使用http
一个Pod(DaemonSet只一个容器)以共享主机网络名称空间在节点上运行，就不用Service,这个Daemon节点运行负载均衡器，如nginx,使用路径反向代理 
	如节点很多，不必每个节点都有一个DaemonSet，挑出几个(如3个)节点打上污点，只运行负载均衡器，如nginx,做https代理，这3个节点是ingress controller

	
	＃envoy(如做微服务的比较喜欢这个)，Traefik(微服务) 和nginx(后改造的)类似，haProxy最不受用
	k8s目前支持和维护的只有 GCE 和 nginx controllers	
	
有一个Service(可无头)，用标识哪些pod,只起分类作用,不走service ，配置给nginx（请求路径或虚拟主机名),如pod改变IP，Ingress（不是 Ingress Controller）可以读到Service的改IP,并写入Ingress Controller(即nginx)写入nginx配置并reload(如envoy自动检查配置文件变化自动加载)

kubectl explain ingress.spec
	可配置tls (即https ,相当给后端时把https给去了变成http)
	
kubectl explain ingress.spec.rules 
	有host  按主机名分发
	有http 按路径分发

kubectl explain ingress.spec.backend 后端
	有serviceName,servicePort只是找到pod资源
	pod变->serivce变->ingress变 注入ingress controller

kubectl create namespace dev (也可使用yaml文件创建)
kubectl get ns
kubectl delete  ns dev
kubectl apply -f ./ 可以读目录下所有文件 


https://github.com/kubernetes/ingress-nginx/
https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md

https://kubernetes.github.io/ingress-nginx/deploy/
默认watch所有的名称空间，可以加--watch-namespace参数 

https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/cloud/deploy.yaml 中复制内容 并执行 kubectl apply -f deploy.yaml
会下载镜像  docker.io/jettech/kube-webhook-certgen:v1.5.0  下载后用 docker image ls  不会显示docker.io/
			 k8s.gcr.io/ingress-nginx/controller:v0.41.2 下载不了，找到了 docker pull pollyduan/ingress-nginx-controller:v0.41.2
			 docker tag pollyduan/ingress-nginx-controller:v0.41.2  k8s.gcr.io/ingress-nginx/controller:v0.41.2

			看日志 Failed to pull image "k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de": 
			使用 docker tag k8s.gcr.io/ingress-nginx/controller:v0.41.2  k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de 不行 ???,失败镜像也没了？
 			docker image inspect k8s.gcr.io/ingress-nginx/controller:v0.41.2 显示的sha256和日志中不一样？？？
			只能修改deploy.yaml文件内容了,去除＠sha256部分,测试OK
			
			 
名称空间为 ingress-nginx 
kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx --watch
			-A   --all-namespaces
https://kubernetes.github.io/ingress-nginx/deploy/baremetal/ 文档 Bare-metal considerations  组，即只要不是运行在云上或mac上

要运行，内容是 增加一个服务ingress-nginx-controller，Type是NodePort方式,为了可以接入集群外部 
	（还有其它方案Via the host network ，Using a self-provisioned edge ，External IPs）
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/baremetal/deploy.yaml
https://github.com/kubernetes/ingress-nginx/tree/master/deploy/static/provider/baremetal/deploy.yaml 复制方式保存为 baremetal_deploy.yaml ， comment显示为 Release 0.41.2 
kubectl apply -f baremetal_deploy.yaml

baremetal_deploy.yaml 文件可修改为节点端口（即不随机）nodePort:30080  nodePort:30443 
kubectl get svc -n ingress-nginx 看端口，可以curl 仿问那个30080端口 没有反应



要有基础服务
-----vi 11-1-ingress-require.yaml  或 11-2-ingress-require-tomcat.yaml  
apiVersion: v1
kind: Service
metadata:
    #name: myapp
    name: tomcat
    namespace: default
spec:
    selector:
        #app: myapp
        app: tomcat
        release: canary
    ports:
    - name: http
      #port: 80  
      #targetPort: 80
      port: 8080  
      targetPort: 8080  
    - name: ajp
      port: 8009
      targetPort: 8009


---
#这是前面建过的deploy-demo.yaml，一个文件两个区
apiVersion: apps/v1
kind: Deployment
metadata:
  #name: myapp-deploy
  name: tomcat-deploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      #app: myapp #删除kubectl delete rs myapp
      app: tomcat
      release: canary
  template:
    metadata:
      labels:
        #要匹配上面的标签选择器
        #app: myapp
        app: tomcat
        release: canary
    spec:
      containers:
     # - name: myapp
     #   image: nginx:latest
      - name: tomcat
        image: tomcat:9.0.24-jdk8
        ports:
        - name: http
          #containerPort: 80
          containerPort: 8080
        - name: ajp
          containerPort: 8009
 
 
-----vi 11-3-ingress-myapp.yaml   或 11-4-ingress-tomcat.yaml  
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: ingress-myapp
    #name: ingress-tomcat
    namespace: default
    annotations:
        kubernetes.io/ingress.class: "nginx"  #指定ingress是用nginx
spec:
    #tls:
    #- hosts:
    #   - tomcat.mycluster
    #  secretName: tomcat-ingress-secret #下面创建的secret tls
    rules:
    - host: myapp.mycluster #要是互联网上可解析的主机名,也可用路径来做
    #- host: tomcat.mycluster #这里是80端口不能加:8080
      http:
         paths:
         -  path: #如host有值，这里可没值/testpath
            backend:
              serviceName: myapp
              servicePort: 80
              #serviceName: tomcat
              #servicePort: 8080
         #-  path: #tomcat可同时暴露两个端口
         #   backend: 
         #     serviceName: tomcat
         #     servicePort: 8009

-----
kubectl apply -f 11-3-ingress-myapp.yaml
kubectl get ingress  
kubectl describe ingress  ingress-myapp  可以到主机名到后端
kubectl get pods -n ingress-nginx

看nginx配置文件 
kubectl exec -n ingress-nginx -it ingress-nginx-controllerxxx -- /bin/sh 
$ cat nginx.conf 中有 server_name myapp.mycluster ;  但没有upstream 的从节点IP ？？？？  但测试页面成功

集群外机器 vi /etc/hosts
<从节点1 IP> myapp.mycluster 
<从节点2 IP> myapp.mycluster 
 集群外访问  myapp.mycluster:30080 有页面，测试成功
  

https自签证书（生产要做ca)
openssl genrsa -out tls.key 2048 私钥
openssl req  -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Shanghai/L=JiaDing/O=DevOps/CN=myapp.mycluster 自签证书
CN的值要和前面一致
要转换才行 隐藏一些东西，只是base64转了一下，可以轻松反转
kubectl create secret tls tomcat-ingress-secret --cert tls.crt --key=tls.key
kubectl get secret
kubectl describe secret tomcat-ingress-secret 

重建 Ingress (11-3-ingress-myapp.yaml   或 11-4-ingress-tomcat.yaml  ) 看Ingress controller的nginx发现有 443在监听

/etc/hosts 每个从节点加 tomcat.mycluster

测试 http://myapp.mycluster:30080  可以看到nginx首页
测试 https://tomcat.mycluster:30443 有tomcat主页,浏览器打开成功，用curl命令不接受不可信证书


---卷
pod的卷 其实是里面全部容器(docker)都复制 pause 容器的卷/网络 （docker image ls 有pause）

emptyDir 卷，如pod删除，数据也删除，数据目录可放内存中 
网络位置 NFS，CIFS，iSCSI,分布式存储GlusterFS,cephfs,云存储

kubectl explain pods.spec.volumes 
显示支持的存储方式，有 emptyDir，hostPath(节点上的目录),nfs,iscsi,rdb(ceph块存储)，cephfs,glusterfs,cinder(openStack上的)，persistentVolumeClaim(PVC),nfs,gitRepo，iscsi ... 
 
kubectl explain pods.spec.volumes.emptyDir
	medium 如为""表示存硬盘，如为Memory 表示使用内存
	sizeLimit 使用上限
	
kubectl explain pods.spec.containers.volumeMount
	有 mountPath,readOnly,name(volume name)


	
--vi 12-pod-vol-deom.yaml 
#复制前面的pod-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-vol-demo
  namespace: default
  labels:
    app: myapp
    tier: frontend
  #annotations:
  #  created-by: cluster admin
spec:
  containers:
  - name: myapp
    image: nginx:latest  
    volumeMounts:
    - name: html-vol
      mountPath: /data/web/html
  - name: busybox
    image: busybox:latest
    #imagePullPolicy: IfNotPresent
    command:
    -  "/bin/sh"
    -  "-c"
    -  "while true; do echo $(date)>>/data/index.html;sleep 2;done "
    #command: ["/bin/sh","-c","touch /tmp/healthy;sleep 30 ; rm -f /tmp/healthy;sleep 360"]
    volumeMounts:
    - name: html-vol
      mountPath: /data/     
  volumes:
  - name: html-vol
    emptyDir:
    #hostPath:
    #   path: /data/ 
    #   type: DirectoryOrCreate  #可取值为 DirectoryOrCreate,Direcotry,FileOrCreate,File...
    #nfs:
    #   path: /nfs_dir/
    #   server: store01.myDomain
    #   readOnly: false
	 

kubectl explain pvc.spec (persistentVolumeClaim)
	accessMode 是读还是写 可选值为ReadWriteOnce(RWO)，ReadOnlyMany(ROX),ReadWriteMany(WRX)) 
	resources 存储资源的最小大小
	selector  使用指定标签的pv
	volumeName 使用固定pv 
	
PVC 和 PV 是一对一关系，为了k8s和存储系统间的解耦

mkdir v{1,2,3,4,5}
所有从节点安装nfs-utils
NFS的 /etc/exports
/nfs_dir/v1         192.168.114.0/24(rw,no_root_squash)
/nfs_dir/v2        192.168.114.0/24(rw,no_root_squash)


kubectl explain pv  (persistentVolume)
kubectl explain pv.spec.nfs

PV不能加名称空间，所有名称空间都可用，PVC是用名称空间的

kubectl explain pv.spec 
	URL 看nfs及其它.. 是否支持这三种
	

-----13-pv-demo.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001 
  labels:
    name: pv001
spec:
  #accessModes:
  #- ReadWriteOnce
  #- ReadOnlyMany
  accessModes: ["ReadWriteOnce","ReadOnlyMany"]
  capacity: 
    storage: 2Gi
  nfs:
    path: /nfs_dir/v1
    server: store01.myDomain
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv002 
  labels:
    name: pv002
spec:
  accessModes:
  - ReadOnlyMany
  capacity: 
    storage: 1Gi
  nfs:
    path: /nfs_dir/v2
    server: store01.myDomain 	
----
kubectl apply -f 13-pv-demo.yaml 
kubectl get pv 
	有RECLAIM POLICY列 Retain保留，即pvc不关联pv，pv以前的数据怎么办，也可是recycle就是数据全删了，delete就是删pv，这两个删比较危险



PVC 的 accessModes 要是 PV 的 accessModes 的子集才行，即PV要能符合PVC的条件
-----14-pod-pvc-demo.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc 
  namespace: default 
spec:
  #accessModes:
  #- ReadWriteOnce 
  accessModes: ["ReadWriteOnce"] #选择PV的条件
  resources: 
    requests: 
      storage: 1G
  #storageClassName: #也要和PV相同 动态创建PV,要求存储服务支持resetful接口
  #selector: #可以选择快慢的
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-vol-pvc 
  namespace: default 
spec:
  containers:
  - name: myapp
    image: nginx:latest 
    volumeMounts:
    - name: html-vol
      mountPath: /data/web/html
  volumes:
  - name: html-vol
    persistentVolumeClaim:  
        claimName: mypvc
        readOnly: false
-----
kubectl apply -f 14-pod-pvc-demo.yaml 
kubectl get pvc    如是Pending 查pv的CLAIM列就是有值，不能很好的释放，只能重新PV
kubectl describe pods pod-vol-pvc  看

PVC不在节点上，是存在etcd上的  
---PVC 根据storageClassName 动态创建PV,要求存储服务支持resetful接口,如ceph, glusterfs 简单但不支持resetful,要找第三方
要先搭个ceph
在您使用 Ceph 卷之前，您的 Ceph 服务器必须正常运行并且要使用的 share 被导出（exported）

https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs
https://github.com/kubernetes/examples/tree/master/volumes/cephfs/ 

https://kubernetes.io/zh/docs/concepts/storage/storage-classes/
https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#ceph-rbd 

https://kubernetes.io/zh/docs/concepts/storage/dynamic-provisioning/

---secret  configMap
kuernetes 向容器传环境变量 
secret(base64编码的) 和 configMap(明文存放的) 是用来做读取配置用的(配置中心),configMap可当存储卷用
	
kubectl explain pods.spec.containers 有env ,envFrom
kubectl explain pods.spec.containers.envFrom 有configMapKeyRef,feildRef 引用已经配置的字段，SecretKeyRef

configMap 是key,value对，value没有长度限制可以是整个配置文件 (配置文件中有引号可能不行)

kubectl explain configmap 
kubectl explain cm

可以不使用yaml创建
kubectl create configmap --help有示例
--from-literal 直接从命令行读数据
--from-file=key1=/file1.txt 
--from-file=/path/file2.txt  没有key就使用文件名当key

kubectl create configmap nginx-config --from-literal=nginx_port=80 --from-literal=server_name=node01.mydomain
kubectl get  cm  nginx-config 
kubectl describe  cm  nginx-config 

---vi www.conf
server{
	server_name node01.mydomain;
	listen 80;
	root /data	
}
---
kubectl create configmap www-config --from-file=www=www.conf
kubectl get  cm  www-config -o yaml  显示有很多\n
kubectl describe  cm  www-config  看到清楚，没有\n


----- vi 15-pod-cm.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-cm-1 
  namespace: default 
spec:
  containers:
  - name: myapp
    image: nginx:latest
    ports:
    - name: http
      containerPort: 80
    env:
    - name: NGINX_SERVER_PORT  #定要用_不能用- 这个变量应该是容器能读和处理
      valueFrom: 
         configMapKeyRef: 
           name: nginx-config  #是config map的名字
           key: nginx_port
           optional: true
    - name: MYSQL_ROOT_PASS 
      valueFrom:
        secretKeyRef:
          name: mysql-root-pass  #后面创建的，到pod中的环境变量是明文的
          key: pwd
    volumeMounts:
    - name: nginxconfig  #引用卷名
      mountPath: /etc/nginx/conf.d
      readOnly: true
  volumes:
  - name: nginxconfig
    configMap:
      name:  www-config  #是config map的名字,当键是文件名，值是文件内容
       #items: #只要部分而不是全部，多个键时 即生成 多个文件时 
       #-  key:   #必填
       #  path:   #必填 把值当做什么文件名，文件路径不能用..开头
       #  mode:  #值当做文件的权限 如0777 
       
---

kubectl edit  cm  nginx-config  打开vi编辑环境变量后，进入pod中查 env命令 后容器不会立即生效，是pod创建时取的值
		如是编辑挂载方式的configMap可以生效的，要等一会才行

--vi 15-2-my-cloud-config-dev.yml
#kubectl create namespace my-ns
apiVersion: v1
kind: Namespace
metadata:
  name: my-ns
  #labels:
    #aa: bb
---
apiVersion: v1
kind: ConfigMap 
metadata:
  name: my-cloud-config
  namespace: my-ns
data:
  application.yaml: |-
    spring:
      profiles: dev
    myprop:
      greeting: Say Hello to the Developers
      name: prop_val_dev
--
kubectl apply -f 15-2my-cloud-config-dev.yml 
		

kubelet create secret --help 有三种类型,子命令为
	docker-registry
	generic
	tls

kubectl explain pods.spec 有一个 imagePullSecret

kubectl create secret generic mysql-root-pass --from-literal=pwd=root123

kubectl get secret mysql-root-pass 显示类型是Opaque
kubectl describe secret mysql-root-pass 看不到值
kubectl describe secret mysql-root-pass -o yaml 有显示加密后的值，是base64编码  #新版本没有-o选项

echo TXlQQHNzMTIz | base64 -d 显示base64解码后的原值



---StatefulSet
有状态的，如redis,mysql 比较麻烦的

要求有持久存储
启动关闭服务要有顺序
要求无头服务，请求直接到pod，（如service有clusterIP，生成的pod没办法控制顺序）有唯一的网络标识
要求使用volumeClaimTemplate 每个pod有自己的存储卷PVC,PV (如使用pod template 中的volume，多个pod使用同一个template就是同一个volume，就不行)

kubectl explain sts(SatefulSet)

----vi 16-stateful-demo.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp 
  labels:
    app: myapp
spec:
  selector: 
     app: myapp-pod 
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http
    port: 80 
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp
spec: 
   serviceName: myapp   #无头服务
   replicas: 3
   selector:
     matchLabels:
       app: myapp-pod
   template:
     metadata:
       labels:
         app: myapp-pod
     spec:
       containers:
       - name: myapp
         image: nginx:latest
         ports:
         - containerPort: 80
           name: web
         volumeMounts:
         - name: myappdata
           mountPath: /usr/share/nginx/html
   volumeClaimTemplates: #会为每个pod生成一个PVC
   - metadata:
       name: myappdata
     spec:
       accessModes: ["ReadWriteOnce"] 
       resources:
         requests:
           storage: 1Gi
----
可以一次删多个pod/deploy 
kubectl delete pods/pod-cm-2 pods/pod-secre-1
kubectl delete deploy/xxx deploy/yy
kubectl delete pv --all

事先建好PV
kubectl get sts 显示Desired数（replicas: 3）
kubectl get pvc 行数和sts 的Desired同，显示的NAME列以myappdata-myapp-1 即 卷名-pod名  ,Bound(bind的过去式)表示在用，绑定到了pv上
kubectl get pods 显示3个pod名是是有顺序的(要有足够的pv可用)
如pod删除 pvc还在

kubectl describe  pods myapp-2 显示用的ClaimName就是myappdata-myapp-2，删除pod再建也是同一个

进入myapp-0 (kubectl exec  -it myapp-0 -- /bin/sh)后
	nslookup myapp-1.myapp.default.svc.cluster.local 是可以被解析的 
	dig -t A  myapp-1.myapp.default.svc.cluster.local @10.96.0.10
 格式是 pod名.无头服务名

StatefulSet 也支持扩容，滚动更新
kubectl scale sts myapp --replicas=5
kubectl patch sts myapp -p '{"spect":{"replicas":2}}'


kubectl explain sts.spec 有 updateStrategy
kubectl explain sts.spec.updateStrategy.rollingUpdate 有partition,默认为0,如定义有3，表示myapp-3和后面的>3的做更新

如partition 是5个,partition为5只更新一个，模拟金丝雀，如没问题，再把myapp-5修改为myapp-0，partition修改为0


kubectl patch sts myapp -p '{"spec":{"updateStrategy":{"RollingUpdate":{"partition",4}}}}'
kubectl patch sts myapp -p '{"spec":{"template":{"spec":{"container[0]":{"image":"nginx:1.17.5-alpine"}}}}}'

也可升级版本
kubectl set image sts/myapp myapp=nginx:1.17.5-alpine 
kubectl get pods myapp-0 -o yaml 看版本
kubectl get sts -o wide

无状态应用应该是像redis这种，网上找别人写好的,不是好的作法
kubectl delete  pvc myappdata-myapp-0
kubectl delete  pvc myappdata-myapp-1

-------serviceaccount

kubeadm 的~/.kub/config文件中有认证信息 
kubectl  proxy  --port=8080 启动了一代理服务器 
kubectl get deploy  -n kube-system 

kubectl get svc 有一个 kubernetes 的 clusterIP 为10.96.0.1为群集内网地址
kubectl describe  svc kubernetes 看EndPoint是外网地址

kubectl explain pods.spec  有一个 serviceAccountName,建立的pods有，volume 类型为Secret,SecretName值为default-token-xxx
每个名称空间都一个secret,为pod连接api-server用，默认这个secret只能仿问自己的pod

kubectl create serviceaccount mysa -o yaml --dry-run=client (不真的执行)
#--dry-run 过时用 --dry-run=client
 -o yaml 显示出使用yaml建立的文件内容，比较少，可直接复制 

kubectl get pods myapp  -o yaml --export 比较全  status只读数据，就不要了

kubectl create serviceaccount admin 
kubectl get sa (serviceaccount) 
kubectl describe sa  admin	 会自动生成一个token,只能登录，但没权限
	有Image pull Secret字段
----17-pod-sa-demo.yaml 
#kubectl create serviceaccount admin 
#kubectl create serviceaccount admin -o yaml --dry-run=client 

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  annotations:
    a: b
  labels:
    c: d
  namespace: default
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-sa-demo
spec:
  containers:
  - name: myapp
    image: localhost:5000/myapp:v3
    ports:
    - name: http
      containerPort: 80
  serviceAccountName: admin
-----
	imagePullSecrets 不太好，还是使用sa上的Image pull Secret比较好
	
kubectl config view  其实~/.kube/config
	context表示 每个context连接一个集群的帐号
(redact 编辑，编写)
格式
contexts:
- context:
		cluster:是clusters中的名字
		user: 是users中的名字
	name: context名1
current-context: context名1

clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.42.129:6443
  name: 默认值 kubernetes， 对应于context.cluster 的名字

users:
- name: 以应于context.user 的名字
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED


kubectl config --help 有很多子命令来创建
/etc/kubernetes/pki/下有 ca.crt和ca.key ，还有很多证书

生成私钥  
(umask 077; openssl genrsa -out my.key 2048) 为了安全使用子shell ，umask设置了有效范围

基于私钥生成证书签署请求由 ca.cert来签 ,-subj "/CN=的值要和用户帐号一样 
openssl req -new -key my.key -o my.csr -subj "/CN=lisi"
开始签证
openssl x509 -req -in my.csr -CA ./ca.crt -CAKey ./ca.key -CAcreateserial -out my.crt -days 366

openssl x509  -in my.crt --text -noout

kubectl config set-credentials lisi --client-certificate=./my.crt --client-key=./my.key --embed-certs=true

kubectl config set-context lisi@kubernetes --cluster=kubernetes --user=lisi

#kubectl config use-context lisi@kubernetes 设置后就没权限了，后要再修改回

kubectl config --help 
kubectl config set-cluser --help 
kubectl config get-contexts 显示所有可用的context

kubectl config --kubeconfig 指定文件路径 默认为~/.kube/config
kubectl config set-cluser myCluster --kubeconfig=/tmp/test.conf  --server=https://1.2.3.4:6443 --certificate-authority=/etc/kubernetes/pki/ca.cert --embed-certs=true
	
kubectl config  view --kubeconfig=/tmp/test.conf 

----- role based acccess control (RBAC) 
Role  只参当前名称空间 使用 RoleBinding  
ClusterRole  对全部名称空间 使用 ClusterRoleBinding,RoleBinding(是对RoleBinding所在名称空间有效,好处是不用重复在每个名称空间定义相同角色)

kubectl create role --help
kubectl create role  pod-reader --verb=get,list,watch --resource=pods --dry-run=client -o yaml >role-demo.yaml

#--dry-run 过时了用 --dry-run=client
也可使用 --verb=get --verb=list --verb=watch

--vi 18-demo-role.yaml 
#kubectl create role  pod-reader --verb=get,list,watch --resource=pods --dry-run=client -o yaml >role-demo.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pods-reader
  #namespace: default
rules:
- apiGroups:
  - ""
  resources:
   - pods
  verbs:
    - get
    - list
    - watch
   
--
kubectl apply -f 18-role-demo.yaml
kubectl get role
kubectl descripbe role pods-reader

kubectl create rolebinding --help
kubectl create rolebinding user1-read-pods --role=pods-readers --user=user1 -o yaml --dry-run=client > rolebinding-demo.yaml


--vi 19-rolebinding-demo.yaml

#kubectl create rolebinding user1-read-pods --role=pods-readers --user=user1 -o yaml --dry-run=client > rolebinding-demo.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: user1-read-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-readers
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User #可为Group,SerivceAccount
  name: user1 
---

kubectl explain user 报不存在，其实user没有资源
测试用户权限,kubectl config  view 查看当前用户，切换context，use-context

kubectl create clusterrole --help

kubectl create clusterrole cluster-reader --verb=get,list,watch  --resource=pods -o yaml --dry-run=client > clusterrole-demo.yaml
--vi 20-clusterrole-demo.yaml
#kubectl create clusterrole cluster-reader --verb=get,list,watch  --resource=pods -o yaml --dry-run=client > clusterrole-demo.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: cluster-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch

---

kubectl create clusterrolebinding user1-read-all-pods --clusterrole=cluser-reader --user=user1 --dry-run=client -o yaml  

--vi 21-clusterrolebinding-demo.yaml
#kubectl create clusterrolebinding user1-read-all-pods --clusterrole=cluser-reader --user=user1 --dry-run=client -o yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: user1-read-all-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluser-reader
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user1
---
kubelet get clusterrole 报错？？有很多系统自带的，如admin,cluster-admin
kubelet get clusterrole user1 可以看到有哪些资源，权限可用   
kubectl describe clusterrolebinding user1-read-all-pods 
kubectl get clusterrolebinding  有 cluster-admin
kubectl delete clusterrolebinding  user1-read-all-pods   
测试rolebinding到一个clusterrole



kubectl create rolebinding user1-read-pods --clusterrole=cluster-readers --user=user1 -o ymal --dry-run > rolebinding-clusterrole-demo.yaml

kubectl get clusterrolebinding cluster-admin
显示 是一个Group叫system:master
kube config view 显示当前登录用户，kubernetes-admin
用户所属组定义在证书中 openssl x509 -in /etc/kubernetes/pki/apiserver-kubelet-client.crt -text -noout
	Subject: O=system:master(组),CN=kube-apiserver-kubelet-client （说是kubernetes-admin的CN是这个）

kubectl get pods -n kube-system 显示有flannel的，看安装时的yaml 定的角色
kubectl get pods kube-flannel-ds-amd64-xxx -n kube-system -o yaml  看有serviceAccountName的定义
容器启动时以这个serviceAccount与apiserver通讯

-------- 新的dashboard 2.0.3 兼容 k8s-1.18
https://github.com/kubernetes/dashboard
https://github.com/kubernetes/dashboard/blob/master/aio/deploy/recommended.yaml

mkdir $HOME/certs && cd $HOME/certs
openssl genrsa -out tls.key 2048 私钥
openssl req  -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Shanghai/L=JiaDing/O=DevOps/CN=myapp.mycluster 自签证书

Deployment 中 修改 (可能不用这步)
	containers:
      - args: 下增加
        - --tls-cert-file=/root/certs/tls.crt
        - --tls-key-file=/root/certs/tls.key

kubectl create --edit -f  recommended.yaml  会把第一个区打开编辑，会把所有区依次打开
kubectl create -f  recommended.yaml


docker pull kubernetesui/dashboard:v2.0.3   虽是中央仓库但还是很慢

docker image save -o kube_dashboard-2.0.3.tar  kubernetesui/dashboard:v2.0.3 
docker load -i kube_dashboard-2.0.3.tar  

docker pull kubernetesui/metrics-scraper:v1.0.4  
docker image save -o kube_metrics-scraper-1.0.4.tar kubernetesui/metrics-scraper:v1.0.4  
docker load -i kube_metrics-scraper-1.0.4.tar


kubectl -n kubernetes-dashboard get pod

 
kubectl -n kubernetes-dashboard get svc  是clusterIP类型，可修改为nodePort类型
kubectl patch svc kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}'  -n kubernetes-dashboard 
再 kubectl -n kubernetes-dashboard get svc  查使用自动端口 443:31395
htts协议来仿问  htts://外网IP:31395

可token认证 ，也要先创建 serviceacount,
	kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard 
	kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 
	kubectl get secret -n kubernetes-dashboard   找 dashboard-admin-token-xxx 是建dashboard-admin生成的 
	kubectl describe secret dashboard-admin-token-xxx  -n kubernetes-dashboard  里面有token信息,复制到登录页中（测试成功）
	
方式二
	kubectl create serviceaccount def-ns-admin -n default   #只管理default名称空间，使用rolebinding
	kubectl create rolebinding def-ns-admin --clusterrole=admin --serviceaccount=default:def-ns-admin
	#kubectl delete rolebinding def-ns-admin  
	#
	kubectl create clusterrolebinding def-cluster-admin --clusterrole=cluster-admin  --serviceaccount=default:def-ns-admin
	#kubectl delete clusterrolebinding def-cluster-admin  
		
		
 	kubectl get secret | awk '/^def-ns-admin/{print $1}'
	kubectl describe secret  def-ns-admin-token-xxx (也可用里面的token登录，测试成功)

	kubectl get secret -n default -o jsonpath={.items[0].data.token} | base64 -d   是base64加密的，解密可登录
 
登录的token 使用做config命令的set-credentials
kubectl get secret -n default -o json 返回数组,找有 def-ns-admin-token-xxx 组中的 token(结尾有==是base64加密的，解密可登录 )

#jsonpath 文档 https://goessner.net/articles/JsonPath/  
kubectl get secret -n default 有 def-ns-admin-token-xxx
kubectl get secret  def-ns-admin-token-xxx -n default -o jsonpath={.data.token}  
还是复制到DEF_NS_AMDIN_TOKEN变量中吧
#DEF_NS_AMDIN_TOKEN=(kubectl get secret def-ns-admin-token-xxx -n default -o jsonpath={.data.token} | base64 -d)  

cd /etc/kubernetes/pki/

kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server="https://<api-server>:6443" --embed-certs=true --kubeconfig=/root/def-ns-admin.conf  #生成文件，起cluster名为kubernetes

kubectl config view --kubeconfig=/root/def-ns-admin.conf

kubectl config set-credentials def-ns-admin --kubeconfig=/root/def-ns-admin.conf --token=$DEF_NS_AMDIN_TOKEN ##前面的要base64解码
 #文件中增加了用户 def-ns-admin ，密码为xxx
 
kubectl config set-context def-ns-admin@kubernetes --cluster=kubernetes --user=def-ns-admin  --kubeconfig=/root/def-ns-admin.conf
#文件中context把user和cluster做关联

kubectl config use-context def-ns-admin@kubernetes --kubeconfig=/root/def-ns-admin.conf  #设置当前使用的context
就可以使用/root/def-ns-admin.conf文件登录了 (测试成功)

客户端使用格式
kubectl get pod  --kubeconfig=/C/app/tmp/vm_k8s_config 

----


kubectl get configmap -n kube-system

kubectl get configmap -n kube-system kube-proxy -o yaml  把mode修改为ipvs,(pod与 service通信)，ipvs(ipvsadm包)没有取代iptables


1.容器间通讯 同个pods的多个容器，使用lo
2.pod通讯 直接使用IP
3.pod与service通讯 ,podIp <-> clusterIP (iptables转)
4.service与集群外部通信，使用 ingress,nodePort,

CNI = ContainerNetworkInterface 实现
	calico (印花布，白棉布) 复杂， 能实现网络策略,可不让他管理网络使用flannel来做
	flannel (绒布,毛巾)简单，不能实现网络策略，各名称空间的网络隔离 
	canel
	可以 calico + flannel 一起用
	
解决方案 bridge,多路利用macVlan
/etc/cni/net.d/目录下的配置文件为CNI插件
 cat  /etc/cni/net.d/10-flannel.conflist  
  hairpin 发夹
  
---------flannel
3层网格 fabric (织物 结构)
https://github.com/coreos/flannel
 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
 
 
 
ip a s 有 
	flannel.1 网络接口 (ip地址是0.0/32,mtu 1450,正常是1500,少50做叠加) , 是隧道
	cni0 网络接口 ,在 flannel.1  之上 (10.244.0.1/24 )

flannel支持 VxLan 叠加隧道 （对于不在同一网段），如是同在网段使用host-gateway


flannel 是一个 DaemonSet（每个节点只可运行一个）可共享主机网络名称空间

kubectl get daemonset  -n kube-system
显示DESIRED 数为  pods的数量 和节点数相同
kubectl get pods  -n kube-system -o wide  确认是在每个节点上


kubectl get configmap -n kube-system 有一个 kube-flannel-cfg

kubectl get configmap kube-flannel-cfg -o json  -n kube-system
kubectl get configmap kube-flannel-cfg -o yaml  -n kube-system
有Network的IP信息,"Backend": { "Type":  "vxlan   

两个节点 进入shell 进行ping操作，抓包观察
tcpdump -i ens32 -nn icmp 是抓不到的 
ip a s 显示veth开头的网卡 是什么另外一半，被桥接到 cni0上的
brctl show cni0


tcpdump -i cni0 -nn icmp 抓是有的 报文进来是从cni0进来，再到flannel.1(已经是vxLan报文) ， 再从物理网卡 ens32发出


tcpdump -i flannel.1 -nn  

tcpdump -i ens32 -nn host <经过的IP> 可以看到OTV overlay的网络 

修改flannel为 Directrouting 为true 默认为flase
方式一 暂时不知道如何做 vi net-conf.json  安装flannel时的yaml文件中也有这部分
{
	"NetWork": "10.244.0.0/16",
	"Backend": {
		"Type": "vxlan", 
		"Directrouting": true
	}
}
方式二 也可用 kubectl edit configmap kube-flannel-cfg -n kube-system 里面有类似上面的东西，这里像用vi一样修改，如有语法错误:wq后最上面有提示，测试失败？？？
ip route show 看已经生成规则 但没有生效（有使用flannel.1 没有使用ens32，没有直接使用物理网卡通信）

route -n  (centOS没有这个命令？？)

kubectl get configmap kube-flannel-cfg -o json -n kube-system  显示是否有 "Directrouting": true

kubectl get pods -n kube-system -w 看kube-flannel开头状态，当重新安装 flannel(kubectl delete -f xx,kubectl apply -f xx)
重建deployment应用
ip route show 已经生效，正常应该是先建立flannel再建deployment应用
两个节点再ping ,tcpdump -i ens32 -nn icmp 就没有overlay网络了，如不是一个网段自动overlay网络

"Type":"host-gw" 各节点不能跨网段

---------calico
https://www.projectcalico.org/  三色猫 

Installing Calico for policy (advanced)
https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/other

calico 也使用 etcd，通过 api-server 来操作kubernetes的etcd

安装 calico  
curl https://docs.projectcalico.org/v3.10/manifests/calico-policy-only.yaml -O

kubectl apply -f  calico-policy-only.yaml

kubectl get pods -n kube-system 	有calico-开头的
kubectl get pods -n kube-system -w 等全部为Runing

---------

egress 出路，出口
ingress 入口 
网络规则很像iptables

kubectl explain networkpolicy
kubectl explain networkpolicy.spec 有 egress , ingress ,podSelector 对哪些pod
	policyTypes 可取值为 "Ingress", "Egress", 或 "Ingress,Egress"，表示哪个生效，如不指定值那么存在的就生效

kubectl explain networkpolicy.spec
kubectl explain networkpolicy.spec.egress.to 有  namespaceSelector,ipBlock,podSelector

--vi 22-1-ingress-def-deny-all.yaml 
#拒绝全部进入
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
spec:
  podSelector: {} #空表示选中全部
  policyTypes:
    - Ingress   #这type为Ingress表示控制，又没有定 表示全部拒绝进入，type没有Egress表示不控制,也没定义表示全部允许出去
     
--vi 22-2-ingress-def-allow-all.yaml 
#允许全部进入 
apiVersion: networking.k8s.io/v1  
kind: NetworkPolicy
metadata:
  name: allow-all-ingress  
spec:
  podSelector: {} 
  ingress:
    - {}  #定义了是空 表示允许全部
  policyTypes:
    - Ingress   
    
----
kubectl create namespace dev

kubectl create namespace prod

kubectl apply -f 22-1-ingress-def-deny-all.yaml -n dev #指定名称空间
kubectl get  netpol -n dev #有 deny-all-ingress ,netpol 的全称是 netpolicy

--vi 22-0-pod1.yaml
apiVersion: v1   
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: myapp
    image: nginx:latest

---
kubectl apply -f  22-0-pod1.yaml -n dev
kubectl get  pods -n dev -o wide 有显示IP
curl 显示IP ，前面NetworkPolicy的限制  ，正常应该是不通的，测试不行？？？ 

再建prod空间,没有定义网络限制
kubectl apply -f  22-0-pod1.yaml -n prod 
kubectl get  pods -n prod -o wide  有显示IP 如果有两个从节点，就可能会运行在不同的节点上
curl 显示IP 

为pos打标签为选中用
kubectl label pods pod1 app=myapp -n dev 


--vi 23-allow-netpol-demo.yaml  
apiVersion: networking.k8s.io/v1  
kind: NetworkPolicy
metadata:
  name: allow-myapp-ingress
spec:
  podSelector: 
    matchLabels:
      app: myapp 
  ingress:
  - from: 
    - ipBlock:
        cidr: 10.244.1.6/16 #<IP>/<子网>
        except:
        - 10.244.1.7/17 #<IP>/<子网>
    ports:
    - protocol: TCP
      port: 80    
----
CIDR 全称是无分类域间路由选择，英文全称是Classless Inter-Domain Routing
CIDR子网划分

kubectl apply -f allow-netpol-demo.yaml  -n dev

--------调度器schedular

选择节点运行pod, 要先过滤符合条件的 预选(predict),再选最好的(priority),再选定,因有可能两个节点的先级一样（select)

有些pod希望运行在同一节点上(同一位置)，像不要距离太远,叫 亲和性
有些不希望运行在一个节点上(同一位置)，像有故障不能全部不能用，端口冲突，有机密数据，叫 反亲和性

一些节点可以有污点（taint），pod可以定义容忍(tolerance)，即pod容忍一定要多于节点污点数才会在上面运行，如不想让pod在节点上运行，只要在节点增加污点即

https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/algorithm 下有 predicates目录 和priorities 目录
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/algorithm/predicates/predicates.go  定义了很多预选，但不是每个都会被启用
	像func NoDiskConflict 上也有注释


kubectl explain pods.spec   
	 nodeName(pod只能运行在这个节点上)
	 hostName  # 如果pod上定义hostName的值，会检查和节点的hostname是否匹配 （并不是必须，表示节点上不能重名的pod）
	 nodeSelector 如多个是并且的关系
	
预选策略 
	1.CheckNodeCondition
	2.GeneralPredicates 包含多个的子的 
		1)	HostName (检查pod上是否定义pods.spec.hostName)
		2)	PodFitsHostPorts (检查节点有可用的端口，即 kubectl explain pods.spec.containers.ports.hostPort)
		3)	MatchNodeSelector  即 kubectl explain pods.spec.nodeSelector
		4)	PodFitsResources 检查cpu,内存等，使用 kubectl get nodes 再用 kubectl describe nodes node1 看cpu,memory
	3.NoDiskConflict  (默认没有启用)存储卷满足要求
	4.PodToleratesNodeTaints 检查  kubectl explain pods.spec.tolerations  可接受节点的污点
	4.PodToleratesNodeNoExecuteTaints 检查NoExecute属性，当节点后加污点不满足pod的容忍时，默认pod还是继续在节点上运行(有可能有些东西还在运行)，如NoExecute就不在节点上运行
	5.CheckNodeLabelPresence (默认没有启用)标签满足要求
	6.CheckServiceAffinity (默认没有启用) (affinity密切关系) 把新的pod放在已经相同service的pod所在的节点上运行
	
	MaxEBSVolumeCount		是亚马逊的
	MaxGCEPDVolumeCount  是Goole的
	MaxAzureDiskVolumeCount  是微软的
	
	7.CheckVolumeBinding  检查节点绑定或未绑定的PVC(persistentVolumeClaim) 
	8.NoVolumeZoneConflict 
	9.MatchInterPodAffinity 

优选
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/algorithm/priorities/priorities.go

	1.LeastRequestedPriority  最少请求的(pod要求的cpu和内存) 
	2.BalancedResourceAllocation  均衡使用cpu和内存
	3.NodePreferAvoidPodsPriority 没有pod的节点，节点的annotation信息 schedular.alpha.kubernetes.io/preferAvoidPods (不确定是对的) 如没有，得分是10満分
	4.TaintTolerationPriority   如pod的toleration和节点的taint匹配的越多，分越低
	5.SelectorSpreadPriority   节点上已经运行的pod符合标签选择器的越少，得分越高
	6.InterPodAffinityPriority  亲和
	7.NodeAffinityPriority 		根据pods的nodeSelector 亲和
	8.MostRequestedPriority 	(默认没有启用) 与 LeastRequestedPriority 相反
	9.ImageLocalityPriority  	(默认没有启用) 节点已经有镜像(体积大小之和) 高分
	
--vi 24-pod-sche.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: myapp
spec:
  containers:
  - name: myapp
    image: nginx:latest
  nodeSelector:
     diskType: ssd                      
--
kubectl apply -f 24-pod-sche.yaml 

kubectl  get nodes  --show-labels

如没有节点的标签符合pods 
查 kubectl  get pods   状态是PENDING   , kubectl  describe pods xx 看原因

kubectl label node node1 diskType=ssd
kubectl label node node1 diskType-  删标签

kubectl explain pods.spec.affinity 

kubectl explain pods.spec.affinity.nodeAffinity
 	preferredDuringSchedulingIgnoredDuringExecution 表示首选，不满足也可以 ， 而required必须满足

kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields

---nodeAffinity节点亲和

-- vi 25-1-node-affinity-required.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-required
  labels:
    app: myapp
spec:
  containers:
  - name: myapp
    image: nginx:latest
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values: 
            - foo
            - bar            
--

kubectl explain pods.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution

-- vi 25-2-node-affinity-preferred.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-preferred
  labels:
    app: myapp
spec:
  containers:
    - name: myapp
      image: nginx:latest
  affinity:
    nodeAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
     - preference:
         matchExpressions:
         - key: zone
           operator: In
           values:
            - foo
            - bar
       weight: 60

 
--
pod亲和 一组pod第一个在哪个机架上，其它的pod也要在这个机架上,如一套服务的nginx,java,mysql在一个局部网

kubectl explain pods.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution
	有 labelSelector 用来选择哪些pods 
		namespaces 选择标签是哪个名称空间，如为空就是这个pods的名称空间
		topologyKey 用来选择哪些node ,如同机架(rack)上的
		
co-locate = affinity

kubectl get nodes --show-labels 
发现每个节点都有标签kubernetes.io/hostname=<hostname的值> 可用于 topologyKey

 

------26-pods-affinity-required.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: pods-frontend
  labels:
    app: myapp
    tire: frontend
spec:
  containers:
    - name: myapp
      image: nginx:latest
---
apiVersion: v1
kind: Pod
metadata:
  name: pods-db
  labels:
    app: redis
    tire: db
spec:
#  containers:
#    - name: redis
#      image: redis:latest
  containers:
  - name: busybox
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["sh","-c","sleep 3600"]
  affinity:
    podAffinity:
    #podAntiAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
     - labelSelector:
         matchExpressions:
         -  {key: app, operator: In, values: ["myapp"]}
       topologyKey: kubernetes.io/hostname
       
------ 
kubectl get pods -o wide 都运行在一个节点上(pods-frontend所在的)
kubectl describe pods pods-db 有日志指定到哪个节点

kubectl explain pods.spec.affinity.podAntiAffinity. 反亲和 
如把所有节点使用相同标签，topologyKey配置这个标签,再立就会有Pendding状态 (required),如为prefered就可运行

节点的三类键值对，1.label,2.annotation,3.taint(只能用于节点上)


---节点污点,pod容忍 
kubectl get nodes node1 -o yaml 也有spec:
kubectl explain node.spec 有 taints 数组
kubectl explain node.spec.taints 
有key,value(可为空)定义，effect 如不能容忍污点时的行为 
	NoSchedule 对已经运行的pod的，后来修改节点的污点，已有的pod不做调度(修改)，新的pods只影响调度
	PreferNoSchedule  新的pods最好不要运行在这个节点，有些情况(没有资源)也可运行
	NoExecute 现有pod有影响，不执行,去pod(杀pod)

查节点的污点
kubectl describe nodes node1  找 Taints 
 	master节点默认有一个，就是为了不能运行普通pod,但要能运行内部pod 

kubectl get pods -n kube-system 
kubectl describe pods kube-proxy-xxxx -n kube-system  找Tolerations 

语法
kubectl taint nodes node1 key1=value1:NoSchedule  key2=value2:NoSchedule

kubectl taint nodes node1 runEnv=prod:NoSchedule --overwrite  只对相同的Type相同的Key,否则就新建

删除污点
kubectl taint nodes node1 runEnv=prod:NoSchedule-


如pod没有定义容忍，就不能运行在已经有污点的节点上

#kubectl explain deployment.spec.template.metadata 

kubectl explain pod.spec.tolerations
	如effect为空 表示容忍全部
 有tolerationSeconds 如果pod被杀要等一会(只对NoExecute有效)，默认为永远，不evict 
 	 operator 有 Equal(默认),Exists
 	 
--vi 27-pod-tolerate-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-tolerate-demo
  namespace: default
spec:
  replicas: 2
  selector: 
    matchLabels:
      app: myapp #删除kubectl delete rs myapp
      realease: canary 
  template:
    metadata:
      labels:
        app: myapp  
        realease: canary 
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
      tolerations:
      - key: "runEnv"
        operator: "Equal"
        value: "prod"
        effect: "NoExecute"
        tolerationSeconds: 20
----- 

测试下来不只是key=value 匹配， 还要effect也要两匹配才行

----------pod资源限制
request 应用运行要有这些
limit 应用最大只能使用这些

2核4线程CPU 就 虚拟成4个逻辑CPU

1逻辑CPU = 1000 的毫核millcores
0.5cpu=500m


内存Ei,Pi,Ti,Gi,Mi

kubectl explain pods.spec.containers 
kubectl explain pods.spec.containers.resources
有limits , requests


删除污点，只对Key
kubectl taint nodes node1 runEnv-


------vi 28-pods-cpu-limits.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pods-cpu-limits
  labels:
    app: myapp
    tire: frontend
spec:
  containers:
    - name: stress-ng
      image: ikubernetes/stress-ng:latest
      command: ["/usr/bin/stress-ng","-c 1","--metrics-brief" ] #-c1 表示对cpu使用1个worker测试 -m 1表示对内存  
      resources:
        requests:
          cpu: "200m"
          memory: "128Mi"
        #limits: #QoS Class:       Burstable
        #  cpu: "500m"  #一半的cpu
        #  memory: "200Mi"
        limits:  #QoS Class:       Guaranteed
          cpu: "200m"
          memory: "128Mi"
------
建立后
kubectl exec pods-cpu-limits -- top 
	看到CPU使用了13% 原因是节点是4核，500m用8/1 即12.5%(如是2核就是25%)，这里看到的数是节点的资源而不是容器的

kubectl describe pods pods-cpu-limits 
显示 Qos (Quality Of Service) class:  Burstable
	Guranteed 表示每个容器设置了cpu,memory并且 limits.memory=requests.memory并且 limits.cpu=requests.cpu 优先级最高
	Burstable(稳定) 至少一个容器设置requests
	BestEffort 没有任何一个容器设置了request 或 limits ，优先级最低

如一个节点资源不够用，就会kill BestEffort低优先级类型的pod，给高优先级的用
再Burstable类型就是request资源占用比例较大(使用量接近request)删

-----------job
#https://kubernetes.io/zh/docs/concepts/workloads/controllers/job/
---29_job.yml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  template:
    spec:
      containers:
      - name: myjob
        image: busybox
        command: ["/bin/sh", "-c", "date;echo $(date) >> /tmp/job.log"]
      restartPolicy: Never #OnFailure
  backoffLimit: 4  #最多失败多少次
  
  
  		  
kubectl get job
kubectl describe  job  myjob 显示  Created pod
 
 注意，即使你将 .spec.parallelism 设置为 1，且将 .spec.completions 设置为 1，并且 .spec.template.spec.restartPolicy 设置为 "Never"，同一程序仍然有可能被启动两次

像做压力测试一样，如.spec.parallelism 设置为2 .spec.completions 设置为8，每次启动2个pod(2个一起结束)，共运行4次
  
 
  
-----------cronjob
#https://kubernetes.io/zh/docs/concepts/workloads/controllers/cron-jobs/
--30_cron_job.y
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  #分，时，日，月，周，@daily (or @midnight)相当于 0 0 * * *
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - date;echo $(date) >> /tmp/cron.log
          restartPolicy: OnFailure
		  
		  
		  
kubectl get cj
kubectl describe cj hello 显示 Created job xxx

某些情况下，可能会创建两个 Job，或者不会创建任何 Job
时间都是基于 kube-controller-manager. 的时区

concurrencyPolicy 默认为 Allow，表示如下次时间到了，即使上一下还没有运行完，也要执行


spec.successfulJobsHistoryLimit  默认最多保存3个成功的历史
spec.startingDeadlineSeconds 最晚启动时间为多少秒


-----------自定义k8s资源
https://github.com/kubernetes/sample-controller/blob/master/artifacts/examples/crd.yaml

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
spec:
  group: samplecontroller.k8s.io
  version: v1alpha1
  names:
    kind: Foo
    plural: foos
  scope: Namespaced


----
kubectl apply -f crd.yaml     

kubectl get crd



---- example-foo.yaml
apiVersion: samplecontroller.k8s.io/v1alpha1  #对应上面的group/version
kind: Foo
metadata:
  name: example-foo
spec:
  deploymentName: example-foo
  replicas: 1
--
kubectl create -f artifacts/examples/example-foo.yaml

kubectl get foo


go env -w GOPROXY=https://goproxy.cn,direct
编译示例项目  go build -o sample-controller  .




----------metrics-server

https://github.com/kubernetes-sigs/metrics-server
项目 的deploy/1.8+/目录下有很多yaml文件
看 metrics-server-deployment.yaml  有tmp-dir 的卷是emptyDir的配置项，生产环境要做修改。
      volumes: 
      - name: tmp-dir
        emptyDir: {}

有镜像   image: k8s.gcr.io/metrics-server-amd64:v0.3.6   
docker pull mirrorgooglecontainers/metrics-server-amd64:v0.3.6
docker tag 	mirrorgooglecontainers/metrics-server-amd64:v0.3.6   k8s.gcr.io/metrics-server-amd64:v0.3.6


cd deploy/1.8+/
kubectl apply -f  .  #表示当前目录下所有文件

kubectl get svc -n kube-system
kubectl get pod -n kube-system 显示失败 

kubectl describe pod metrics-server-xxxx -n kube-system 
下载镜像 失败 原因为metrics-server-deployment.yaml 中  imagePullPolicy: Always 修改为 IfNotPresent

kubectl api-versions 显示有 metrics.k8s.io/v1beta1

kubectl proxy --port 8080 反代理 直接从api-server取数据

curl http://127.0.0.1:8080/apis/metrics.k8s.io/v1beta1  (/api/后是kubectl api-versions 中的)有返回 监控的数据
curl http://127.0.0.1:8080/apis/metrics.k8s.io/v1beta1/nodes  有返回，但没数据 "items": []  
curl http://127.0.0.1:8080/apis/metrics.k8s.io/v1beta1/pods   有返回，但没数据 "items": []

kubectl top node 还没数据

kubectl logs  metrics-server-xxxx -c metrics-server -n kube-system  显示有报错
	-c, --container

kubectl logs  -f --tail=100 pod_xxx -n my-ns | tee pod.log
kubectl logs  -f --tail=100 "$(kubectl get pods -l app=pod_xxx  -n my-ns -o name)" -n my-ns | tee pod_xxx.log
kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}' 也可只显示pod名字

docker image inspect mirrorgooglecontainers/metrics-server-amd64:v0.3.6
看启动命令是 /metric-server

修改metrics-server-deployment.yaml  在image:下面的 args增加一个
	 - --source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true&kuberletPort=10250&insecure=true
重建后就有节点数据了
curl http://127.0.0.1:8080/apis/metrics.k8s.io/v1beta1/nodes 
kubectl top nodes

要有运行的pod 下面才会有数据
kubectl top pods
kubectl top pods -n kube-system


/var/log/containers/ 目录是节点的日志信息

--------------helm 掌舵，掌管
使用go语言写的 ， 简化kubernetes安装(像yum)

https://helm.sh/
https://github.com/helm/helm 
	在release标签 中下载 Linux amd64 二进制包helm-v2.16.0-linux-amd64.tar.gz，解压后就两个命令 ,helm,tiler

charts 是一组所有需要的yaml文件如deployment,service ,有模板和 config值 生成release, 不需要镜像仓库，用私有镜像仓库即可
charts  也有仓库，安装在api-server上要经过 tiler  （小船的）舵柄； 耕种者

helm -> tiler ->api server

helm init --service-account <NAME>
./helm init --service-account tiler 		会创建~/.helm目录  要在kubectl可运行的节点上运行

kubectl get svc -n kube-system 查到有 tiller-deploy 
kubectl get pod -n kube-system  没有？？？

./helm version 看客户端版本和服务端版本，服务端不行？？？

./helm  repo update

官方可用chart列表
https://hub.kubeapps.com/   搜索redis  看说明

./helm search redis  结果类似上面的网址
./helm inspect stable/redis
./helm install --name ch_redis stable/redis   可以加-f --values 指定值文件
./helm list
./helm delete ch_redis


ls ~/.helm/cache/archive/redis-9.5.2.tgz 
  里面templates目录是定义的yaml模板 和 的values.yaml默认值

./helm create myapp  生成了myapp目录及相关文件

--------------名称空间的资源限制

kubectl create quota quota-test --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4 --namespace=my-ns
kubectl  delete quota quota-test -n my-ns


--方式二  vim quota-test.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-test
  namespace: my-ns
spec:
  hard:
    requests.cpu: "2"
    requests.memory: 2Gi
    limits.cpu: "4"
    limits.memory: 4Gi
    requests.nvidia.com/gpu: 4
    pods: "3"
    services: "6"
	

kubectl create -f quota-test.yaml

kubectl describe ns my-ns

kubectl get deploy xxx -o yaml 看status区的日志
--------



 